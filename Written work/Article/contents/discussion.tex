\section{Discussion}
\label{sec:discussion}

The chosen approach to handle the imbalanced classes was to randomly down-sample to fit the minority class. By during so, it is possible to discard value information, e.g. the spatial variation within each class. Although, during down-sampling randomly for several training epochs, the different permutation of variations for each class will be captured.  
An alternate approach to fix the imbalanced classes is to penalize the loss function w.r.t. to the class distribution in each mini-batch.

In \cite{	main_ar,VGGnet16} they subtract the mean image. The normalization of the images have not been implemented in this current state of the project, which is a candidate for the low average performance.

The size of the mini-batch was chosen to be 32. This hyperparameter variates within the literature but due to the static implementation of the LSTM cell, the discarded images in the final mini-batch needs to as few as possible. A lower batch size will entail an faster converging of the weights. 

Although the CNN reduces spatial variance, the temporal variance of the images can be huge. Two consecutive images, illustrating the sleep epochs, can have many displacements in the objects. The implementation of the LSTM cell is inspired by video frame prediction \cite{git_lstm}, which has fewer displacements in the objects for two consecutive frames. 

One of the challenge issues with the used data is, that the annotation of the sleep stages requires expert skilled personal. There has been applied several experts, where each was responsible of the annotation for a subgroup of the subjects. This entail that the networks are fitting to each experts subjective experiences. The quality of the annotations could be enhanced by introduce more experts to each of the subjects.


Learning rate in the LSTM celll is maybe to low.. Therefore an analysis of the learning curve can contribute to the more knowlede....


Analysis the learning curve.... How much does the performance measures change as a function of the amount of training? Bacicallly how much better will the classifier be, if we doubled the number of training epochs.

