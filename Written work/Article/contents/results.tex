
\section{Experimental Evaluation}
\subsection{Setup}


The setup from training the baseline model and the RNN model is identically. Both model are based upon layers from the VGGNet 16 model. This model have already been trained on ILSVRC data set for several weeks and it is beneficial to applied those archived weights and using the principle of transfer learning instead of the learning all the weights from scratch. 

According to the literature \cite{stand_cnn_notes_1}, there are following approaches to do transfer-learning: Remove the final fully connected layer of the network. Use the pre-trained CNN as an feature extractor for the new fully connected layer, which fits the data set according to number of classes. The second strategy is carry out the first strategy and fine-tune some of the weights in the pre-trained CNN.
The chosen strategy in this setup was to remove and create a new final fully connected layer, which fits the data set for both networks. Instead of fine-tuning the weights in the other fully connected layers, it has been chosen to train the dropout operations between those layers. This will prevent overfitting. 
The weights in the LSTM cell have been trained from scratch. The weights in the forget bias gate have been initialized to $1$, which causes the LSTM cell does not have any prior knowledge.

This strategy is different compared to the setup in \cite{main_ar}. They re-train the all the weights in the last three fully connected layers.

During the training process, the networks have only access to the current epoch. This is different from the setup in \cite{main_ar}, where the network uses two prior epochs and two posterior epochs in order to learn the current epoch. The reason for this choice was, real time considerations. If the classifier should work in real time, then it does not have any posterior epochs to work with. It is possible to use prior epochs in real time classifications, and may be considered as future research.

Both networks have been optimized in order to minimize their categorical cross-entropy. Their loss function is handled by the AdamOptimizer provided by TF iteration over mini-batches of 32. The training process have been repeated for 20 training epochs. The learning rate and its decay rate are given in table \ref{tab_hyper}.

Due the characteristic of the sleep stages, the classes (table \ref{tab_class_balance}) are not equally distributed. The selected choice for fixing this issue was to randomly downsample the majority classes to fit the minority class in each training epoch, as they did in \cite{main_ar}. 

The setup is capable of performing leave-one-out cross-validation due to the relative small number of subjects and few professionel skilled sleep stage experts.
The subject have been divided into test, train and validation. The validation subjects have been fixed to subject 19-20. The test subject for first fold i subject one and train data includes subjects two-18. Due to time was only chosen to perform the first fold.

\subsection{Results}
\label{subsec:results}

Table \ref{tab_res_1} summerieas the performance, evaluated for the two validation subjects, for both networks. The first column block reports the confusion matrix for the validation subjects. The next block reports the normalized confusion matrix. The third block report the following per-class metrics: Precision, sensitivity, F$_1$-score and accuracy. The accuracy is not a reliable metric for the performance in this project, yield to misleading results of the imbalanced classes in the validation set \cite[sec. 11]{dl_book}.

\begin{table*}[th!]
\centering
\begin{tabular}{ll | llllll | llllll | llll}
                     &    & \multicolumn{6}{c}{Predicted} & \multicolumn{6}{| c}{Normalized pred. (in \%)}  & \multicolumn{4}{| c}{Per-class metric (in \%)} \\
                     &    & W  & N1  & N2  & N3  & N4 & R & W & N1 & N2 & N3 & N4 & R & Pre.       & Sen.      & F$_1$      & Acc.      \\\hline
\multirow{6}{*}{CNN} & W  &495 & 145 & 29 & 11 & 1 & 20 & 71 & 21 & 4 & 2 & 0 & 3 & 91 & 71 & 80 & 93 \\ 
                     & N1 &    25 & 211 & 43 & 0 & 0 & 62 & 7 & 62 & 13 & 0 & 0 & 18 & 43 & 62 & 51 & 89 \\ 
                     & N2 &    4 & 51 & 1313 & 104 & 17 & 68 & 0 & 3 & 84 & 7 & 1 & 4 & 91 & 84 & 88 & 90 \\ 
                     & N3 &    0 & 2 & 11 & 164 & 64 & 0 & 0 & 1 & 5 & 68 & 27 & 0 & 49 & 68 & 57 & 93 \\ 
                     & N4 &    0 & 0 & 0 & 54 & 91 & 0 & 0 & 0 & 0 & 37 & 63 & 0 & 53 & 63 & 57 & 96 \\ 
                     & R  &    17 & 80 & 46 & 0 & 0 & 591 & 2 & 11 & 6 & 0 & 0 & 81 & 80 & 81 & 80 & 92 \\ \hline
\multirow{6}{*}{RNN} & W  &    578 & 39 & 26 & 7 & 1 & 43 & 83 & 6 & 4 & 1 & 0 & 6 & 89 & 83 & 86 & 95 \\ 
                     & N1 &    38 & 107 & 64 & 0 & 0 & 132 & 11 & 31 & 19 & 0 & 0 & 39 & 55 & 31 & 40 & 91 \\ 
                     & N2 &    8 & 13 & 1314 & 102 & 28 & 92 & 1 & 1 & 84 & 7 & 2 & 6 & 90 & 84 & 87 & 89 \\ 
                     & N3 &    3 & 0 & 18 & 125 & 95 & 0 & 1 & 0 & 7 & 52 & 39 & 0 & 43 & 52 & 47 & 92 \\ 
                     & N4 &    0 & 0 & 1 & 60 & 84 & 0 & 0 & 0 & 1 & 41 & 58 & 0 & 40 & 58 & 48 & 95 \\ 
                     & R  &    19 & 36 & 43 & 0 & 0 & 636 & 3 & 5 & 6 & 0 & 0 & 87 & 70 & 87 & 78 & 90
\end{tabular}
\caption{My caption}
\label{tab_res_1}
\end{table*}

precision: fraction which were deteced correct.
sensitivity: fraction of 



\begin{table}[th!]
\centering
\begin{tabular}{l | llll}
Study & Precision & Sensitivity & F$_1$-score & Accuracy \\\hline
%\cite{main_ar}               & 65.4-\textbf{67.9}-70.4 & 70.9-\textbf{71.3}-71.8 & 67.5-\textbf{68.8}-70.0 & 92.3-\textbf{92.3}-92.4\\
CNN               & 65.4-\textbf{67.9}-70.4 & 70.9-\textbf{71.3}-71.8 & 67.5-\textbf{68.8}-70.0 & 92.3-\textbf{92.3}-92.4\\
RNN               & 61.9-\textbf{64.6}-67.2 & 63.2-\textbf{65.9}-68.6 & 61.8-\textbf{64.2}-66.6 & 92.2-\textbf{92.2}-92.2
\end{tabular}
\caption{My caption}
\label{tb_res_2}
\end{table}




\subsubsection{sensitivity}


\begin{figure*}[th!]
\centering
\input{./contents/subfigs}
\caption{This figure contains plots of each annotated sleeping stage for subject 1. The plots are given columnwise from left to right according to the previous mentioned sequence of the sleeping stages. Fig. \ref{fig_1_11} to \ref{fig_1_16} illustrates an random epoch of the multi-taper spectrum for each sleeping stage for subset 1. It is clear to see a high similarity of sleeping stage N3 and N4, fig. \ref{fig_1_14} and fig. \ref{fig_1_15}. Second and third row, fig \ref{fig_1_21} to \ref{fig_1_36} shows the sensitivity maps from the CNN and the RNN respectively.}
\label{fig_1}
\end{figure*}



\subsubsection{Hypnograms}


\begin{figure*}[th!]
\centering
\input{./contents/subfigs_hyp}
\caption{A figure with two subfigures}
\label{fig_hyp}
\end{figure*}

give examples where it totally wrong?


figure \ref{fig_hyp_3} around $t_{800}$... something wired is going on in the RNN




