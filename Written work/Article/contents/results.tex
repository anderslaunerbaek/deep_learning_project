
\section{Experimental Evaluation}
\subsection{Setup}


The setup from training the baseline model and the RNN model is identically. Both model are based upon layers from the VGGNet 16 model. This model have already been trained on ILSVRC data set for several weeks and it is beneficial to applied those archived weights and using the principle of transfer learning instead of the learning all the weights from scratch. 

According to the literature \cite{stand_cnn_notes_1}, there are following approaches to do transfer-learning: Remove the final fully connected layer of the network. Use the pre-trained CNN as an feature extractor for the new fully connected layer, which fits the data set according to number of classes. The second strategy is carry out the first strategy and fine-tune some of the weights in the pre-trained CNN.
The chosen strategy in this setup was to remove and create a new final fully connected layer, which fits the data set for both networks. Instead of fine-tuning the weights in the other fully connected layers, it has been chosen to train the dropout operations between those layers. This will prevent overfitting. 
The weights in the LSTM cell have been trained from scratch. The weights in the forget bias gate have been initialized to $1$, which causes the LSTM cell does not have any prior knowledge.

This strategy is different compared to the setup in \cite{main_ar}. They re-train the all the weights in the last three fully connected layers.

During the training process, the networks have only access to the current epoch. This is different from the setup in \cite{main_ar}, where the network uses two prior epochs and two posterior epochs in order to learn the current epoch. The reason for this choice was, real time considerations. If the classifier should work in real time, then it does not have any posterior epochs to work with. It is possible to use prior epochs in real time classifications, and may be considered as future research.

Both networks have been optimized in order to minimize their categorical cross-entropy. Their loss function is handled by the AdamOptimizer provided by TF iteration over mini-batches of 32. The training process have been repeated for 20 training epochs. The learning rate and its decay rate are given in table \ref{tab_hyper}.

Due the characteristic of the sleep stages, the classes (table \ref{tab_class_balance}) are not equally distributed. The selected choice for fixing this issue was to randomly downsample the majority classes to fit the minority class in each training epoch, as they did in \cite{main_ar}. 

The setup is capable of performing leave-one-out cross-validation due to the relative small number of subjects and few professionel skilled sleep stage experts.
The subject have been divided into test, train and validation. The validation subjects have been fixed to subject 19-20. The test subject for first fold i subject one and train data includes subjects two-18. Due to time was only chosen to perform the first fold.

\subsection{Results}
\label{subsec:results}

Table \ref{tab_res_1} summerieas the performance, evaluated for the two validation subjects, for both networks. The first column block reports the confusion matrix for the validation subjects. The next block reports the normalized confusion matrix. The third block report the following per-class metrics: Precision, sensitivity, F$_1$-score and accuracy. The accuracy is not a reliable metric for the performance in this project, yield to misleading results of the imbalanced classes in the validation set \cite[sec. 11]{dl_book}.

\begin{table*}[th!]
\centering
\begin{tabular}{ll | llllll | llllll | llll}
                     &    & \multicolumn{6}{c}{Predicted} & \multicolumn{6}{| c}{Normalized pred. (in \%)}  & \multicolumn{4}{| c}{Per-class metric (in \%)} \\
                     &    & W  & N1  & N2  & N3  & N4 & R & W & N1 & N2 & N3 & N4 & R & Pre.       & Sen.      & F$_1$      & Acc.      \\\hline
\multirow{6}{*}{CNN} & W  &495 & 145 & 29 & 11 & 1 & 20 & 71 & 21 & 4 & 2 & 0 & 3 & 91 & 71 & 80 & 93 \\ 
                     & N1 &    25 & 211 & 43 & 0 & 0 & 62 & 7 & 62 & 13 & 0 & 0 & 18 & 43 & 62 & 51 & 89 \\ 
                     & N2 &    4 & 51 & 1313 & 104 & 17 & 68 & 0 & 3 & 84 & 7 & 1 & 4 & 91 & 84 & 88 & 90 \\ 
                     & N3 &    0 & 2 & 11 & 164 & 64 & 0 & 0 & 1 & 5 & 68 & 27 & 0 & 49 & 68 & 57 & 93 \\ 
                     & N4 &    0 & 0 & 0 & 54 & 91 & 0 & 0 & 0 & 0 & 37 & 63 & 0 & 53 & 63 & 57 & 96 \\ 
                     & R  &    17 & 80 & 46 & 0 & 0 & 591 & 2 & 11 & 6 & 0 & 0 & 81 & 80 & 81 & 80 & 92 \\ \hline
\multirow{6}{*}{RNN} & W  &    578 & 39 & 26 & 7 & 1 & 43 & 83 & 6 & 4 & 1 & 0 & 6 & 89 & 83 & 86 & 95 \\ 
                     & N1 &    38 & 107 & 64 & 0 & 0 & 132 & 11 & 31 & 19 & 0 & 0 & 39 & 55 & 31 & 40 & 91 \\ 
                     & N2 &    8 & 13 & 1314 & 102 & 28 & 92 & 1 & 1 & 84 & 7 & 2 & 6 & 90 & 84 & 87 & 89 \\ 
                     & N3 &    3 & 0 & 18 & 125 & 95 & 0 & 1 & 0 & 7 & 52 & 39 & 0 & 43 & 52 & 47 & 92 \\ 
                     & N4 &    0 & 0 & 1 & 60 & 84 & 0 & 0 & 0 & 1 & 41 & 58 & 0 & 40 & 58 & 48 & 95 \\ 
                     & R  &    19 & 36 & 43 & 0 & 0 & 636 & 3 & 5 & 6 & 0 & 0 & 87 & 70 & 87 & 78 & 90
\end{tabular}
\caption{This table report the confusion matrix, its normalized confusion matrix and selected performances metrics for the CNN and RNN network.}
\label{tab_res_1}
\end{table*}

The baseline model, reported in table \ref{tab_res_1}, classify the sleeping stage N2 ($84\%$) with the sensitivity. Then followed by R, W, N3, N4 and N1 ($62\%$) as the most difficult sleeping stage to classify. The highest misclassification error is archived in N4 ($27\%$).
The RNN model classifies sleeping stage R ($87\%$) with the highest sensitivity. The followed by N2, W, N4, N3 and N1 ($31\%$) as the most difficult sleeping stage to classify. 



%precision: fraction which were deteced correct.
%sensitivity: fraction of ture events that was detected correct.. percentage of sick people who are correctly identified as having the condition
%F1 score: precision can be traded for sensitivity.. to summerise this score, it is possible to convert the precision and sensitivy into one number -> f1 score..

Table \ref{tab_res_2} reports the mean values and its corresponding $95\%$ confident values computed by bootstrapping. There have been applied $100.000$ bootstrap iterations with replacement in order to compare the average performance of the two models for each metric. 

\begin{table}[th!]
\centering
\begin{tabular}{l | llll}
Study & Precision & Sensitivity & F$_1$-score & Accuracy \\\hline
%\cite{main_ar}               & 65.4-\textbf{67.9}-70.4 & 70.9-\textbf{71.3}-71.8 & 67.5-\textbf{68.8}-70.0 & 92.3-\textbf{92.3}-92.4\\
CNN               & 65-\textbf{68}-70 & 71-\textbf{71}-72 & 67-\textbf{69}-70 & 92-\textbf{92}-92\\
RNN               & 62-\textbf{65}-67 & 63-\textbf{66}-69 & 62-\textbf{64}-67 & 92-\textbf{92}-92
\end{tabular}
\caption{\textbf{Mean} and corresponding $95\%$ confident values computed by 100.000 bootstrap iterations with replacement.}
\label{tab_res_2}
\end{table}

According to table \ref{tab_res_2} can be concluded that the RNN does note out-perform the baseline model w.r.t. the selected measure metrics.


%\subsubsection{Hypnograms}
%\begin{figure*}[th!]
%\centering
%\input{./contents/subfigs_hyp}
%\caption{A figure with two subfigures}
%\label{fig_hyp}
%\end{figure*}
%give examples where it totally wrong?
%figure \ref{fig_hyp_3} around $t_{800}$... something wired is going on in the RNN



\subsubsection{Sensitivity}

The two last rows in figure \ref{fig_1} illustrates the average representation of the six classes, for the two validation subjects. The sensitivity maps have been computed using eq. \ref{eq_1}.

\begin{figure*}[th!]
\centering
\input{./contents/subfigs}
\caption{This figure contains plots of each annotated sleep stage for the two validation subjects. The plots are given columnwise from left to right according to the previous mentioned sequence of the sleeping stages. Fig. \ref{fig_1_11} to \ref{fig_1_16} illustrates an random epoch of the multi-taper spectrum for each sleeping stage. There is high similarity between sleeping stage N3 and N4 (fig. \ref{fig_1_14} and fig. \ref{fig_1_15}). Second and third row, fig \ref{fig_1_21} to \ref{fig_1_36} shows the average sensitivity maps from the CNN and the RNN respectively for the two validation subjects.}
\label{fig_1}
\end{figure*}

Baseline in figure \ref{fig_1_24} and figure \ref{fig_1_25} more less same pattern.. identiticate high similarity between those pictures. - > merge into same class..



\todo[inline]{write some stuff}
