\section{Experimental Evaluation}
\subsection{Setup}

The setup used in training the baseline and the RNN is identical. Both networks are based upon layers from the VGGNet and the weights of the layers are pre-trained on ILSVRC data set for several weeks. It is beneficial to apply the pre-trained weights and use the principle of transfer learning instead of learning the weights from scratch. 

According to the literature \cite{stand_cnn_notes_1}, there are several approaches to perform transfer-learning. Among these is a strategy to remove the final fully connected layer of the network. Then the pre-trained CNN is applied as a feature extractor for a new fully connected layer which fits the number of classes in the data set.
A second strategy is carry out the first strategy and fine-tune a selected set of weights in the pre-trained CNN.

The chosen strategy in this setup was to remove and create a new final fully connected layer which fit the number of sleep stages in both networks. Instead of fine-tuning the weights in the other fully connected layers, it has been chosen to train the dropout operations between those layers in order to prevent overfitting. 

The weights in the LSTM cell have been trained from scratch. The weights in the forget bias have been initialized to $1$ which entails the LSTM cell to have no prior knowledge.
%This strategy is different compared to the setup in \cite{main_ar}. They re-train all the weights in the fully connected layers.

During the training process the networks only have access to the current epoch. This is different from the setup in \cite{main_ar} where the network use two prior epochs and two posterior epochs in order to learn the current epoch. The reason for this choice was due to real time considerations. If the classifier should work in real time then it does not have any posterior epochs to work with. 
It is possible to use prior epochs in real time classifications and it is considered as future research.

Both the CNN and the RNN networks have been optimized in order to minimize their categorical cross-entropy. Their loss function is handled by the AdamOptimizer, provided by TF, which iterates over a mini-batch with a size of 32. 
The training process has been repeated for 20 training epochs. The learning rate has been initialized to $10^{-5}$ and its decay rate, first and second order moment have not been changed from default.

Due to the characteristic of the sleep stages, the sleep stages shown in table \ref{tab_class_balance} are not equally distributed. The selected choice for fixing the issue is to randomly down-sample the majority stages to fit the minority stage in each training epoch. A similar approach has been applied in \cite{main_ar}. 

The setup is capable of performing leave-one-out cross-validation due to the relative small number of subjects and a few professionel skilled sleep stage experts.
The subjects have been divided into test, train and validation. Subject 19 and 20 have been fixed to the validation set in order to get the $\approx10\%$ of the total amount. 

The test subject for the first fold is subject one. The train data includes subject 2 to 18. Due to time considerations it was chosen only to perform the first fold.

\subsection{Results}
\label{subsec:results}

Table \ref{tab_res_1} summarizes the performances evaluated for the two validation subjects in both networks. 
The first column block reports the confusion matrix. The next block reports the normalized confusion matrix. The third block reports the following per-class metrics: Precision, sensitivity, F$_1$-score and accuracy. The accuracy is not a reliable metric of the performance in this project, yield to misleading results of the imbalanced sleep stages in the validation set \cite[sec. 11]{dl_book}.

\begin{table*}[th!]
\centering
\begin{tabular}{ll | llllll | llllll | llll}
                     &    & \multicolumn{6}{c}{Predicted} & \multicolumn{6}{| c}{Normalized pred. (in \%)}  & \multicolumn{4}{| c}{Per-class metric (in \%)} \\
                     &    & W  & N1  & N2  & N3  & N4 & R & W & N1 & N2 & N3 & N4 & R & Pre.       & Sen.      & F$_1$      & Acc.      \\\hline
\multirow{6}{*}{CNN} & W  &495 & 145 & 29 & 11 & 1 & 20 & 71 & 21 & 4 & 2 & 0 & 3 & 91 & 71 & 80 & 93 \\ 
                     & N1 &    25 & 211 & 43 & 0 & 0 & 62 & 7 & 62 & 13 & 0 & 0 & 18 & 43 & 62 & 51 & 89 \\ 
                     & N2 &    4 & 51 & 1313 & 104 & 17 & 68 & 0 & 3 & 84 & 7 & 1 & 4 & 91 & 84 & 88 & 90 \\ 
                     & N3 &    0 & 2 & 11 & 164 & 64 & 0 & 0 & 1 & 5 & 68 & 27 & 0 & 49 & 68 & 57 & 93 \\ 
                     & N4 &    0 & 0 & 0 & 54 & 91 & 0 & 0 & 0 & 0 & 37 & 63 & 0 & 53 & 63 & 57 & 96 \\ 
                     & R  &    17 & 80 & 46 & 0 & 0 & 591 & 2 & 11 & 6 & 0 & 0 & 81 & 80 & 81 & 80 & 92 \\ \hline
\multirow{6}{*}{RNN} & W  &    578 & 39 & 26 & 7 & 1 & 43 & 83 & 6 & 4 & 1 & 0 & 6 & 89 & 83 & 86 & 95 \\ 
                     & N1 &    38 & 107 & 64 & 0 & 0 & 132 & 11 & 31 & 19 & 0 & 0 & 39 & 55 & 31 & 40 & 91 \\ 
                     & N2 &    8 & 13 & 1314 & 102 & 28 & 92 & 1 & 1 & 84 & 7 & 2 & 6 & 90 & 84 & 87 & 89 \\ 
                     & N3 &    3 & 0 & 18 & 125 & 95 & 0 & 1 & 0 & 7 & 52 & 39 & 0 & 43 & 52 & 47 & 92 \\ 
                     & N4 &    0 & 0 & 1 & 60 & 84 & 0 & 0 & 0 & 1 & 41 & 58 & 0 & 40 & 58 & 48 & 95 \\ 
                     & R  &    19 & 36 & 43 & 0 & 0 & 636 & 3 & 5 & 6 & 0 & 0 & 87 & 70 & 87 & 78 & 90
\end{tabular}
\caption{This table report the confusion matrix, its normalized confusion matrix and selected performances metrics for the CNN and RNN network.}
\label{tab_res_1}
\end{table*}

The baseline, reported in table \ref{tab_res_1}, classifies the sleep stage N2 with a sensitivity of $84\%$. Then followed by R, W, N3, N4 and N1. N1 with a sensitivity of $62\%$ as the most difficult sleep stage to classify. The highest misclassification error is achieved in N3 with a sensitivity of $37\%$.

The RNN classifies the sleep stage R with the highest sensitivity ($87\%$). Then followed by N2, W, N4, N3 and N1. N1 with a sensitivity of $31\%$ as the most difficult sleep stage to classify.

The precision metrics for the CNN and the RNN are relatively low for sleep stage N3 and sleep stage N4 compared to the other sleep stages. The misclassification rates for N3 and N4 are relatively close to its classification rates. 
The visual illustrations of the stages are similar, which can be a logical explanation for merging stage N3 and N4 together into one sleep stage as in the newest definition \cite{AASM}.

%precision: fraction which were deteced correct. 
%sensitivity: fraction of ture events that was detected correct.. percentage of sick people who are correctly identified as having the condition.. the probability of a correct detection...
%F1 score: precision can be traded for sensitivity.. to summerise this score, it is possible to convert the precision and sensitivy into one number -> f1 score..

%\todo[inlin]{Precision: Reduce FN and increasing TP. This focus on the cluster of positive..}
%\todo[inlin]{Sensitivity: tries to hard reduce FN and increase TP}
%\todo[inlin]{F1: A combination between precision and sensitivity.}

Table \ref{tab_res_2} reports the mean values and its corresponding $95\%$ confident values computed by bootstrapping. There has been applied $100.000$ bootstrap iterations with replacement in order to compute the average performance metrics of the two networks.
\begin{table}[th!]
\centering
\begin{tabular}{l | llll}
Study & Precision & Sensitivity & F$_1$-score & Accuracy \\\hline
FE \cite{main_ar} & 90-\textbf{91}-93 & 70-\textbf{73}-77 & 78-\textbf{81}-83 & 81-\textbf{83}-85\\
FT \cite{main_ar} & 92-\textbf{93}-94 & 75-\textbf{78}-81 & 82-\textbf{84}-86 & 84-\textbf{86}-88\\\hline
CNN               & 65-\textbf{68}-70 & 71-\textbf{71}-72 & 67-\textbf{69}-70 & 92-\textbf{92}-92\\
RNN               & 62-\textbf{65}-67 & 63-\textbf{66}-69 & 62-\textbf{64}-67 & 92-\textbf{92}-92
\end{tabular}
\caption{\textbf{Mean} and corresponding $95\%$ confident values computed by 100.000 bootstrap iterations with replacement. The two first rows are metrics from \cite{main_ar}.}
\label{tab_res_2}
\end{table}
According to table \ref{tab_res_2} it can be concluded that the RNN does not outperform the baseline network w.r.t. the selected performance metrics. However the RNN captures the transition between stage W and N1 more efficient.

%\subsubsection{Hypnograms}
%\begin{figure*}[th!]
%\centering
%\input{./contents/subfigs_hyp}
%\caption{A figure with two subfigures}
%\label{fig_hyp}
%\end{figure*}
%give examples where it totally wrong?
%figure \ref{fig_hyp_3} around $t_{800}$... something wired is going on in the RNN

\subsubsection{Sensitivity}

The two last rows in fig. \ref{fig_1} illustrate the average representations of the six sleep stages for the two validation subjects. The sensitivity maps have been computed by eq. \ref{eq_1}.

\begin{figure*}[th!]
\centering
\input{./contents/subfigs}
\caption{This figure contain plots of each sleep stage for the two validation subjects. The visualizations are given columnwise from left to right according to the previous sequence of the sleep stages. Fig. \ref{fig_1_11} to \ref{fig_1_16} illustrate a random epoch of the multi-taper spectrum for each of the sleep stages. There is high similarity between sleep stage N3 and N4.
%(fig. \ref{fig_1_14} and fig. \ref{fig_1_15}). 
The second and the third row, fig \ref{fig_1_21} to \ref{fig_1_36} illustrates the average sensitivity maps of the CNN and of the RNN respectively for the two validation subjects.}
\label{fig_1}
\end{figure*}

The sensitivity maps given in fig. \ref{fig_1_24} - \ref{fig_1_25} and fig. \ref{fig_1_34} - \ref{fig_1_35} for the CNN and the RNN respectively have more less the same pattern. 
The summarized information from the sensitivity maps for sleep stage N3 and N4 provides a valid explanation of the high misclassification error in both networks (see the normalized confusion matrices in table \ref{tab_res_1}).

