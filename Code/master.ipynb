{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function \n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join('.', '..')) \n",
    "#import utils\n",
    "import utils_DL\n",
    "import utils_s160159 as u_s\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import sklearn.datasets\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './../Data_1'\n",
    "NUM_SUBJECTS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.4/dist-packages/numpy/core/_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 2...\n"
     ]
    }
   ],
   "source": [
    "#Load all subjects into memory\n",
    "subjects_list = []\n",
    "for i in range(1,NUM_SUBJECTS+1):\n",
    "    print(\"Loading subject %d...\" %(i))\n",
    "    inputs_night1, targets_night1  = u_s.load_spectrograms(data_path=data_dir, subject_id=i, night_id=1)\n",
    "    if(i!=20):\n",
    "        inputs_night2, targets_night2  = u_s.load_spectrograms(data_path=data_dir, subject_id=i, night_id=2)\n",
    "    else:\n",
    "        inputs_night2 = np.empty((0,3,224,224),dtype='uint8')\n",
    "        targets_night2 = np.empty((0,),dtype='uint8')           \n",
    "\n",
    "    current_inputs = np.concatenate((inputs_night1,inputs_night2),axis=0)\n",
    "    current_targets = np.concatenate((targets_night1, targets_night2),axis=0)\n",
    "\n",
    "    mean_images = np.zeros((5,3,224,224))\n",
    "    mean_images = np.zeros((5,224,224,3))\n",
    "    mean_images[0,] = np.mean(current_inputs[current_targets==0,], axis=0)\n",
    "    mean_images[1,] = np.mean(current_inputs[current_targets==1,], axis=0)\n",
    "    mean_images[2,] = np.mean(current_inputs[current_targets==2,], axis=0)\n",
    "    mean_images[3,] = np.mean(current_inputs[np.logical_or(current_targets==3, current_targets==4),], axis=0)\n",
    "    mean_images[4,] = np.mean(current_inputs[current_targets==5,], axis=0)\n",
    "\n",
    "    subjects_list.append([current_inputs,current_targets, mean_images])\n",
    "    \n",
    "    \n",
    "# extract image shapes\n",
    "IMAGE_SHAPE = subjects_list[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict = np.load(data_dir + '/vgg16_weights.npz', encoding='bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperameters\n",
    "NUM_CLASSES = 6\n",
    "HEIGTH, WIDTH, NCHANNELS = IMAGE_SHAPE[1], IMAGE_SHAPE[2], IMAGE_SHAPE[3]\n",
    "PADDING = 'same'\n",
    "L_RATE = 0.00001\n",
    "TRAIN_FEATURES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.cs.toronto.edu/~frossard/vgg16/vgg16.py\n",
    "\n",
    "# Load the weights into memory\n",
    "weights_dict = np.load(data_dir +'vgg16_weights.npz', encoding='bytes')\n",
    "\n",
    "def tf_conv2d(inputs, name, trainable=TRAIN_FEATURES):\n",
    "    with tf.name_scope(name) as scope:\n",
    "        kernel = tf.get_variable(shape=weights_dict[name + '_W'].shape, \n",
    "                                 initializer=tf.constant_initializer(weights_dict[name + '_W']), \n",
    "                                 name=scope + 'weights', \n",
    "                               trainable=trainable)\n",
    "        \n",
    "        conv = tf.nn.conv2d(inputs, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        \n",
    "        biases = tf.get_variable(shape=weights_dict[name + '_b'].shape,\n",
    "                                 initializer=tf.constant_initializer(weights_dict[name + '_b']), \n",
    "                                 trainable=trainable, name=scope + 'biases')\n",
    "\n",
    "        return(tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope))\n",
    "        \n",
    "\n",
    "def tf_max_pooling2d(inputs, name):\n",
    "    with tf.name_scope(name) as scope:\n",
    "        return(tf.nn.max_pool(inputs,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name=scope))        \n",
    "\n",
    "def tf_fully_con(inputs, name, shape=[None, 4096]):\n",
    "    with tf.name_scope(name) as scope:    \n",
    "        fc_W = tf.Variable(tf.truncated_normal(shape, dtype=tf.float32, stddev=1e-1), \n",
    "                            name=scope + 'weights', trainable=True)\n",
    "\n",
    "        fc_b = tf.Variable(tf.constant(0.0, shape=[shape[1]], dtype=tf.float32),\n",
    "                             trainable=True, name=scope + 'biases')\n",
    "        \n",
    "        in_flat = tf.reshape(inputs, [-1, shape[0]])\n",
    "        return(tf.nn.relu(tf.nn.bias_add(tf.matmul(in_flat, fc_W), fc_b)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model consits of  119570438 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "# init model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# init placeholders\n",
    "x_pl = tf.placeholder(tf.float32, [None, HEIGTH, WIDTH, NCHANNELS], name='input_placeholder')\n",
    "y_pl = tf.placeholder(tf.float32, [None, NUM_CLASSES], name='target_placeholder')\n",
    "\n",
    "with tf.variable_scope('VVG16_layer'):\n",
    "\n",
    "    # level one\n",
    "    conv1_1 = tf_conv2d(inputs=x_pl, name='conv1_1')\n",
    "    conv1_2 = tf_conv2d(inputs=conv1_1, name='conv1_2')\n",
    "    pool1 = tf_max_pooling2d(inputs=conv1_2, name='pool1')\n",
    "\n",
    "    # level two\n",
    "    conv2_1 = tf_conv2d(inputs=pool1, name='conv2_1')\n",
    "    conv2_2 = tf_conv2d(inputs=conv2_1, name='conv2_2')\n",
    "    pool2 = tf_max_pooling2d(inputs=conv2_2, name='pool2')\n",
    "\n",
    "    # level three\n",
    "    conv3_1 = tf_conv2d(inputs=pool2, name='conv3_1')\n",
    "    conv3_2 = tf_conv2d(inputs=conv3_1, name='conv3_2')\n",
    "    conv3_3 = tf_conv2d(inputs=conv3_2, name='conv3_3')\n",
    "    pool3 = tf_max_pooling2d(inputs=conv3_3, name='pool_3')\n",
    "\n",
    "    # level four\n",
    "    conv4_1 = tf_conv2d(inputs=pool3, name='conv4_1')\n",
    "    conv4_2 = tf_conv2d(inputs=conv4_1, name='conv4_2')\n",
    "    conv4_3 = tf_conv2d(inputs=conv4_2, name='conv4_3')\n",
    "    pool4 = tf_max_pooling2d(inputs=conv4_3, name='pool_4')\n",
    "\n",
    "    # level five\n",
    "    conv5_1 = tf_conv2d(inputs=pool4, name='conv5_1')\n",
    "    conv5_2 = tf_conv2d(inputs=conv5_1, name='conv5_2')\n",
    "    conv5_3 = tf_conv2d(inputs=conv5_2, name='conv5_3')\n",
    "    pool5 = tf_max_pooling2d(inputs=conv5_3, name='pool_5')\n",
    "\n",
    "    # level six\n",
    "    fc6 = tf_fully_con(inputs=pool5, name='fc6', shape=[int(np.prod(pool5.get_shape()[1:])), 4096])\n",
    "    fc6_dropout = tf.layers.dropout(inputs=fc6, name='fc6_dropout',rate=0.5)\n",
    "\n",
    "    # level seven\n",
    "    fc7 = tf_fully_con(inputs=fc6_dropout, name='fc7', shape=[4096, 4096])\n",
    "    fc7_dropout = tf.layers.dropout(inputs=fc7, name='fc7_dropout',rate=0.5)\n",
    "\n",
    "    # level eigth\n",
    "    fc8 = tf_fully_con(inputs=fc7_dropout, name='fc7', shape=[4096, NUM_CLASSES])\n",
    "    #fc8 = tf_fully_con(inputs=fc7_dropout, name='fc7', shape=[4096, 1000])\n",
    "    \n",
    "with tf.variable_scope('output_layer'):    \n",
    "    l_out = tf.nn.softmax(fc8, name='l_out')\n",
    "\n",
    "print('Model consits of ', utils_DL.num_params(), 'trainable parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Launch TensorBoard, and visualize the TF graph\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.45)\n",
    "\n",
    "#with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "#    tmp_def = utils_DL.rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "#    utils_DL.show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIMISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    # computing cross entropy per sample\n",
    "    cross_entropy = -tf.reduce_sum(y_pl * tf.log(l_out+1e-8), reduction_indices=[1])\n",
    "\n",
    "    # averaging over samples\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "with tf.variable_scope('training'):\n",
    "    # defining our optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=L_RATE)\n",
    "\n",
    "    # applying the gradients\n",
    "    train_op = optimizer.minimize(cross_entropy)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "with tf.variable_scope('performance'):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(l_out, axis=1), tf.argmax(y_pl, axis=1))\n",
    "\n",
    "    # averaging the one-hot encoded vector\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy = tf.reduce_mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test flow for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass    \n",
    "x_batch, y_batch = subjects_list[0][0][0:2], _\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    y_pred = sess.run(fetches=l_out, feed_dict={x_pl: x_batch})\n",
    "\n",
    "assert y_pred.shape == np.zeros((len(x_batch),NUM_CLASSES)).shape, \"ERROR the output shape is not as expected!\" \\\n",
    "        + \" Output shape should be \" + str(l_out.shape) + ' but was ' + str(y_pred.shape)\n",
    "\n",
    "print('Forward pass successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Training Loop\n",
    "batch_size = 100\n",
    "max_epochs = 10\n",
    "\n",
    "\n",
    "valid_loss, valid_accuracy = [], []\n",
    "train_loss, train_accuracy = [], []\n",
    "test_loss, test_accuracy = [], []\n",
    "\n",
    "# reset counter \n",
    "mnist_data.train._epochs_completed = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Begin training loop')\n",
    "\n",
    "    try:\n",
    "        while mnist_data.train.epochs_completed < max_epochs:\n",
    "            _train_loss, _train_accuracy = [], []\n",
    "            \n",
    "            ## Run train op\n",
    "            x_batch, y_batch = mnist_data.train.next_batch(batch_size)\n",
    "            fetches_train = [train_op, cross_entropy, accuracy]\n",
    "            feed_dict_train = {x_pl: x_batch, y_pl: y_batch}\n",
    "            _, _loss, _acc = sess.run(fetches_train, feed_dict_train)\n",
    "            \n",
    "            _train_loss.append(_loss)\n",
    "            _train_accuracy.append(_acc)\n",
    "            \n",
    "\n",
    "            ## Compute validation loss and accuracy\n",
    "            if mnist_data.train.epochs_completed % 1 == 0 \\\n",
    "                    and mnist_data.train._index_in_epoch <= batch_size:\n",
    "                train_loss.append(np.mean(_train_loss))\n",
    "                train_accuracy.append(np.mean(_train_accuracy))\n",
    "\n",
    "                fetches_valid = [cross_entropy, accuracy]\n",
    "                \n",
    "                feed_dict_valid = {x_pl: mnist_data.validation.images, y_pl: mnist_data.validation.labels}\n",
    "                _loss, _acc = sess.run(fetches_valid, feed_dict_valid)\n",
    "                \n",
    "                valid_loss.append(_loss)\n",
    "                valid_accuracy.append(_acc)\n",
    "                print(\"Epoch {} : Train Loss {:6.3f}, Train acc {:6.3f},  Valid loss {:6.3f},  Valid acc {:6.3f}\".format(\n",
    "                    mnist_data.train.epochs_completed, train_loss[-1], train_accuracy[-1], valid_loss[-1], valid_accuracy[-1]))\n",
    "        \n",
    "        \n",
    "        test_epoch = mnist_data.test.epochs_completed\n",
    "        while mnist_data.test.epochs_completed == test_epoch:\n",
    "            x_batch, y_batch = mnist_data.test.next_batch(batch_size)\n",
    "            feed_dict_test = {x_pl: x_batch, y_pl: y_batch}\n",
    "            _loss, _acc = sess.run(fetches_valid, feed_dict_test)\n",
    "            test_loss.append(_loss)\n",
    "            test_accuracy.append(_acc)\n",
    "        print('Test Loss {:6.3f}, Test acc {:6.3f}'.format(\n",
    "                    np.mean(test_loss), np.mean(test_accuracy)))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "epoch = np.arange(len(train_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_accuracy,'r', epoch, valid_accuracy,'b')\n",
    "plt.legend(['Train Acc','Validation Acc'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.75,1.03])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
