{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function \n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.join('.', '..')) \n",
    "import utils\n",
    "import utils_DL\n",
    "import utils_s160159 as u_s\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VERSION = '1.1'\n",
    "FILENAME = 'master'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './../Data'\n",
    "logs_path = './logs'\n",
    "NUM_SUBJECTS = 6\n",
    "NUM_CLASSES = 6\n",
    "VAL_TRAIN_ID = NUM_SUBJECTS - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 6 of 6...\r"
     ]
    }
   ],
   "source": [
    "# Load all subjects into memory\n",
    "subjects_list = []\n",
    "for i in range(1,NUM_SUBJECTS+1):\n",
    "    print(\"Loading subject %d of %d...\" %(i, NUM_SUBJECTS), end='\\r')\n",
    "    inputs_night1, targets_night1, _  = u_s.load_spectrograms(data_path=data_dir, \n",
    "                                                              subject_id=i, \n",
    "                                                              night_id=1,\n",
    "                                                             no_class=NUM_CLASSES)\n",
    "    if i!=20:\n",
    "        inputs_night2, targets_night2, _  = u_s.load_spectrograms(data_path=data_dir, \n",
    "                                                                  subject_id=i, \n",
    "                                                                  night_id=2,\n",
    "                                                             no_class=NUM_CLASSES)\n",
    "    else:\n",
    "        inputs_night2 = np.empty((0,224,224,3),dtype='uint8')\n",
    "        targets_night2 = np.empty((0,NUM_CLASSES),dtype='uint8')           \n",
    "\n",
    "    current_inputs = np.concatenate((inputs_night1,inputs_night2),axis=0)\n",
    "    current_targets = np.concatenate((targets_night1, targets_night2),axis=0)    \n",
    "    subjects_list.append([current_inputs, current_targets])       \n",
    "# extract image shapes\n",
    "IMAGE_SHAPE = subjects_list[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperameters\n",
    "HEIGTH, WIDTH, NCHANNELS = IMAGE_SHAPE[1], IMAGE_SHAPE[2], IMAGE_SHAPE[3]\n",
    "L_RATE = 10e-5\n",
    "L_RATE_MO_1 = 0.9\n",
    "L_RATE_MO_2 = 0.999\n",
    "EPS = 1e-8\n",
    "# Training Loop\n",
    "MAX_EPOCHS = 5 # 50\n",
    "BATCH_SIZE = 75 # 30 works on AWS \n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = False\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.cs.toronto.edu/~frossard/vgg16/vgg16.py\n",
    "# Load the weights into memory\n",
    "weights_dict = np.load(data_dir + '/' + 'vgg16_weights.npz', encoding='bytes')\n",
    "\n",
    "def tf_conv2d(inputs, name):\n",
    "    with tf.name_scope(name) as scope:\n",
    "        weights = tf.get_variable(shape=weights_dict[name + '_W'].shape, \n",
    "                                  initializer=tf.constant_initializer(weights_dict[name + '_W']),\n",
    "                                  name=scope + 'weights', \n",
    "                                  trainable=False)\n",
    "        conv = tf.nn.conv2d(inputs, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.get_variable(shape=weights_dict[name + '_b'].shape,\n",
    "                                 initializer=tf.constant_initializer(weights_dict[name + '_b']), \n",
    "                                 trainable=False, name=scope + 'biases')\n",
    "        return(tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope))\n",
    "        \n",
    "def tf_max_pooling2d(inputs, name, kh = 2, kw = 2, dh = 2, dw = 2):\n",
    "    with tf.name_scope(name) as scope:\n",
    "        return(tf.nn.max_pool(inputs,\n",
    "                              ksize=[1, kh, kw, 1],\n",
    "                              strides=[1, dh, dw, 1],\n",
    "                              padding='VALID',\n",
    "                              name=scope))        \n",
    "\n",
    "def tf_fully_con(inputs, name, n_out=4096, train_able = True):\n",
    "    n_in = n_in = inputs.get_shape()[-1].value\n",
    "    with tf.name_scope(name) as scope:\n",
    "        if train_able:\n",
    "            weights = tf.get_variable(shape=[n_in, n_out],\n",
    "                                      dtype=tf.float32,\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      name=scope + 'weights', \n",
    "                                      trainable=True)\n",
    "\n",
    "            biases = tf.get_variable(shape=n_out,\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=tf.constant_initializer(0.0),\n",
    "                                     trainable=True, \n",
    "                                     name=scope + 'biases')\n",
    "        else:\n",
    "            weights = tf.get_variable(shape=[n_in, n_out],\n",
    "                                      dtype=tf.float32,\n",
    "                                      initializer=tf.constant_initializer(weights_dict[name + '_W']), \n",
    "                                      name=scope + 'weights', \n",
    "                                      trainable=False)\n",
    "\n",
    "            biases = tf.get_variable(shape=n_out,\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=tf.constant_initializer(weights_dict[name + '_b']), \n",
    "                                     trainable=False, \n",
    "                                     name=scope + 'biases')\n",
    "        \n",
    "        #\n",
    "        return(tf.nn.relu(tf.nn.bias_add(tf.matmul(inputs, weights), biases)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace of the tensors shape as it is propagated through the network.\n",
      "Layer name \t Output size\n",
      "--------------------------------------------\n",
      "conv1_1 \t (?, 224, 224, 64)\n",
      "conv1_2 \t (?, 224, 224, 64)\n",
      "pool1 \t\t (?, 112, 112, 64)\n",
      "--------------------------------------------\n",
      "conv2_1 \t (?, 112, 112, 128)\n",
      "conv2_2 \t (?, 112, 112, 128)\n",
      "pool2 \t\t (?, 56, 56, 128)\n",
      "--------------------------------------------\n",
      "conv3_1 \t (?, 56, 56, 256)\n",
      "conv3_2 \t (?, 56, 56, 256)\n",
      "conv3_3 \t (?, 56, 56, 256)\n",
      "pool3 \t\t (?, 28, 28, 256)\n",
      "--------------------------------------------\n",
      "conv4_1 \t (?, 28, 28, 512)\n",
      "conv4_2 \t (?, 28, 28, 512)\n",
      "conv4_3 \t (?, 28, 28, 512)\n",
      "pool4 \t\t (?, 14, 14, 512)\n",
      "--------------------------------------------\n",
      "conv5_1 \t (?, 14, 14, 512)\n",
      "conv5_2 \t (?, 14, 14, 512)\n",
      "conv5_3 \t (?, 14, 14, 512)\n",
      "pool5 \t\t (?, 7, 7, 512)\n",
      "--------------------------------------------\n",
      "flatten \t (?, 25088)\n",
      "fc6 \t\t (?, 4096)\n",
      "fc7 \t\t (?, 4096)\n",
      "fc8 \t\t (?, 6)\n",
      "--------------------------------------------\n",
      "Model consits of  24582 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/huyng/tensorflow-vgg/blob/master/layers.py\n",
    "\n",
    "# init model\n",
    "tf.reset_default_graph()\n",
    "keep_prob = 0.5\n",
    "# init placeholders\n",
    "x_pl = tf.placeholder(tf.float32, [None, HEIGTH, WIDTH, NCHANNELS], name='input_placeholder')\n",
    "y_pl = tf.placeholder(tf.float32, [None, NUM_CLASSES], name='target_placeholder')\n",
    "print('Trace of the tensors shape as it is propagated through the network.')\n",
    "print('Layer name \\t Output size')\n",
    "print('--------------------------------------------')\n",
    "with tf.variable_scope('VVG16_layer'):\n",
    "    # subtract image mean\n",
    "    mu = tf.constant(np.array([115.79640507,127.70359263,119.96839583], dtype=np.float32), \n",
    "                     name=\"rgb_mean\")\n",
    "    net = tf.subtract(x_pl, mu, name=\"input_mean_centered\")\n",
    "    \n",
    "    # level one\n",
    "    net = tf_conv2d(inputs=net, name='conv1_1')\n",
    "    print('conv1_1 \\t', net.get_shape())\n",
    "    net = tf_conv2d(inputs=net, name='conv1_2')\n",
    "    print('conv1_2 \\t', net.get_shape())\n",
    "    net = tf_max_pooling2d(inputs=net, name='pool1')\n",
    "    print('pool1 \\t\\t', net.get_shape())\n",
    "    print('--------------------------------------------')\n",
    "    \n",
    "    # level two\n",
    "    net = tf_conv2d(inputs=net, name='conv2_1')\n",
    "    print('conv2_1 \\t', net.get_shape())\n",
    "    net = tf_conv2d(inputs=net, name='conv2_2')\n",
    "    print('conv2_2 \\t', net.get_shape())\n",
    "    net = tf_max_pooling2d(inputs=net, name='pool2')\n",
    "    print('pool2 \\t\\t', net.get_shape())\n",
    "    print('--------------------------------------------')\n",
    "    \n",
    "    # level three\n",
    "    net = tf_conv2d(inputs=net, name='conv3_1')\n",
    "    print('conv3_1 \\t', net.get_shape())\n",
    "    net = tf_conv2d(inputs=net, name='conv3_2')\n",
    "    print('conv3_2 \\t', net.get_shape())\n",
    "    net = tf_conv2d(inputs=net, name='conv3_3')\n",
    "    print('conv3_3 \\t', net.get_shape())\n",
    "    net = tf_max_pooling2d(inputs=net, name='pool_3')\n",
    "    print('pool3 \\t\\t', net.get_shape())\n",
    "    print('--------------------------------------------')\n",
    "    \n",
    "    # level four\n",
    "    net = tf_conv2d(inputs=net, name='conv4_1')\n",
    "    print('conv4_1 \\t', net.get_shape())\n",
    "    net = tf_conv2d(inputs=net, name='conv4_2')\n",
    "    print('conv4_2 \\t', net.get_shape())\n",
    "    net = tf_conv2d(inputs=net, name='conv4_3')\n",
    "    print('conv4_3 \\t', net.get_shape())\n",
    "    net = tf_max_pooling2d(inputs=net, name='pool_4')\n",
    "    print('pool4 \\t\\t', net.get_shape())\n",
    "    print('--------------------------------------------')\n",
    "\n",
    "    # level five\n",
    "    net = tf_conv2d(inputs=net, name='conv5_1')\n",
    "    print('conv5_1 \\t', net.get_shape())\n",
    "    net = tf_conv2d(inputs=net, name='conv5_2')\n",
    "    print('conv5_2 \\t', net.get_shape())\n",
    "    net = tf_conv2d(inputs=net, name='conv5_3')\n",
    "    print('conv5_3 \\t', net.get_shape())\n",
    "    net = tf_max_pooling2d(inputs=net, name='pool_5')\n",
    "    print('pool5 \\t\\t', net.get_shape())\n",
    "    print('--------------------------------------------')\n",
    "    \n",
    "    \n",
    "    # flatten\n",
    "    flattened_shape = np.prod([s.value for s in net.get_shape()[1:]])\n",
    "    net = tf.reshape(net, [-1, flattened_shape], name=\"flatten\")\n",
    "    print('flatten \\t', net.get_shape())\n",
    "    # level six\n",
    "    net = tf_fully_con(inputs=net, name='fc6', n_out=4096, train_able=False)\n",
    "    print('fc6 \\t\\t', net.get_shape())\n",
    "    net = tf.layers.dropout(inputs=net, name='fc6_dropout', rate=keep_prob)\n",
    "\n",
    "    # level seven\n",
    "    net = tf_fully_con(inputs=net, name='fc7', n_out=4096, train_able=False)\n",
    "    print('fc7 \\t\\t', net.get_shape())\n",
    "    net = tf.layers.dropout(inputs=net, name='fc7_dropout', rate=keep_prob)\n",
    "\n",
    "    # level eigth\n",
    "    logits = tf_fully_con(inputs=net, name='fc8', n_out=NUM_CLASSES)\n",
    "    print('fc8 \\t\\t', logits.get_shape()) \n",
    "    print('--------------------------------------------')\n",
    "    \n",
    "#with tf.variable_scope('output_layer'):\n",
    "#    logits = tf.nn.softmax(net, name='l_out')\n",
    "#    print('out \\t', logits.get_shape())\n",
    "#    print('--------------------------------------------')\n",
    "    \n",
    "#\n",
    "no_train_able = []\n",
    "for i in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='VVG16_layer'):\n",
    "    no_train_able.append(np.prod(i.shape[:]).value)   # i.name if you want just a name   \n",
    "print('Model consits of ', np.sum(no_train_able), 'trainable parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIMISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('performance'):\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    prediction = tf.one_hot(tf.argmax(probs, axis=1), depth=NUM_CLASSES)\n",
    "    prediction_bool = tf.equal(tf.argmax(probs, axis=1), tf.argmax(y_pl, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction_bool, tf.float32))\n",
    "    \n",
    "with tf.variable_scope('loss_function'):\n",
    "    # computing cross entropy\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                            labels=y_pl,\n",
    "                                                            name='cross_entropy')\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    # defining our optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=L_RATE,  \n",
    "                                       beta1=L_RATE_MO_1, \n",
    "                                       beta2=L_RATE_MO_2, \n",
    "                                       epsilon = EPS)\n",
    "    # applying the gradients\n",
    "    train_model = optimizer.minimize(loss)\n",
    "\n",
    "#with tf.variable_scope('sensitivity_map'):\n",
    "#    # https://stackoverflow.com/questions/35226428/how-do-i-get-the-gradient-of-the-loss-at-a-tensorflow-variable\n",
    "#    # https://www.tensorflow.org/versions/r0.12/api_docs/python/train/gradient_computation\n",
    "#    grad_output_wrt_input = tf.gradients(loss, [x_pl],\n",
    "#                                         name='grad_output_wrt_input')[0]\n",
    "#    tf.add_to_collection('grad_output_wrt_input', grad_output_wrt_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test flow for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Launch TensorBoard, and visualize the TF graph\n",
    "# with tf.Session() as sess:\n",
    "    # writer = tf.summary.FileWriter(logs_path, sess.graph)\n",
    "    # close session\n",
    "    # sess.close()\n",
    "# run in terminal\n",
    "# \"\"\"\n",
    "# python -m webbrowser \"http://localhost:6006/\";\n",
    "# tensorboard --logdir='./logs'\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flow test\n",
    "if False:\n",
    "    # Test the forward pass    \n",
    "    x_batch = subjects_list[0][0][0:40]\n",
    "    y_batch = subjects_list[0][1][0:40]\n",
    "\n",
    "    sess = tf.Session(config=config)\n",
    "    #tf.train.start_queue_runners(sess=sess_test)\n",
    "    with sess.as_default():\n",
    "        #\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #\n",
    "        tmp_net = sess.run(fetches=net, \n",
    "                       feed_dict={x_pl: x_batch,\n",
    "                                  y_pl: y_batch})\n",
    "\n",
    "        tmp_pred = sess.run(fetches=prediction, \n",
    "                   feed_dict={x_pl: x_batch})\n",
    "\n",
    "        tmp_pred_cor = sess.run(fetches=prediction_bool, \n",
    "                   feed_dict={x_pl: x_batch,\n",
    "                             y_pl: y_batch})\n",
    "\n",
    "        tmp_accuracy = sess.run(fetches=accuracy, \n",
    "                   feed_dict={x_pl: x_batch,\n",
    "                             y_pl: y_batch})\n",
    "\n",
    "        tmp_cross_entropy = sess.run(fetches=cross_entropy, \n",
    "                   feed_dict={x_pl: x_batch,\n",
    "                             y_pl: y_batch})\n",
    "\n",
    "        tmp_loss = sess.run(fetches=loss, \n",
    "                            feed_dict={x_pl: x_batch,\n",
    "                                      y_pl: y_batch})\n",
    "\n",
    "        #tmp_grad_output_wrt_input = sess.run(fetches=grad_output_wrt_input, \n",
    "        #                    feed_dict={x_pl: x_batch, y_pl: y_batch})\n",
    "\n",
    "        _loss,_acc,_pred = sess.run(fetches=[loss, accuracy, prediction],\n",
    "                            feed_dict={x_pl: x_batch, y_pl: y_batch})\n",
    "        \n",
    "        \n",
    "\n",
    "        #u_s.cal_sen_map(grad_accum=x_batch, IMAGE_SHAPE=IMAGE_SHAPE, sen_map_class='2')\n",
    "        #u_s.cal_sen_map(grad_accum=tmp_grad_output_wrt_input, IMAGE_SHAPE=IMAGE_SHAPE, sen_map_class='2')\n",
    "        #x_batch = subjects_list[0][0][0:100]\n",
    "        #y_batch = subjects_list[0][1][0:100]\n",
    "        #print(time.ctime())\n",
    "        #_,tm2,tm3 = sess.run(fetches=[train_model, loss, accuracy],\n",
    "        #             feed_dict={x_pl: x_batch, y_pl: y_batch})\n",
    "        #print(time.ctime())\n",
    "        #x_batch = subjects_list[0][0][0:1]\n",
    "        #y_batch = subjects_list[0][1][0:1]\n",
    "        #tmp_grad_output_wrt_input = sess.run(fetches=grad_output_wrt_input, \n",
    "        #                    feed_dict={x_pl: x_batch, y_pl: y_batch})\n",
    "        #u_s.cal_sen_map(grad_accum=tmp_grad_output_wrt_input, IMAGE_SHAPE=IMAGE_SHAPE, sen_map_class='2')\n",
    "        #u_s.save_weights(graph= tf.get_default_graph(), fpath=data_dir + '/weigths.npz')\n",
    "        # close session\n",
    "        sess.close()\n",
    "\n",
    "    #assert y_pred.shape == np.zeros((len(x_batch),NUM_CLASSES)).shape, \"ERROR the output shape is not as expected!\" \\\n",
    "    #        + \" Output shape should be \" + str(l_out.shape) + ' but was ' + str(y_pred.shape)\n",
    "\n",
    "    print('Forward pass successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training loop... \n",
      "\n",
      "Fold 1 of 2\n",
      "\tEvaluate test performance\n",
      "\t\tminibatch: 2\tL: 2.538167\tACCs: 0.153333\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cm_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e1a65b810ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m# CAPTURE STATS FOR CURRENT FOLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             capture_dict[fold] = {'cm_test': cm_test,\n\u001b[0;32m---> 77\u001b[0;31m                                   \u001b[0;34m'cm_val'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcm_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                                   \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                                   \u001b[0;34m'train_accuracy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cm_val' is not defined"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    try:\n",
    "        print('Begin training loop... \\n')\n",
    "        \n",
    "        ## INTO VALIDATION\n",
    "        #idx_val = list(range(VAL_TRAIN_ID, NUM_SUBJECTS))\n",
    "        #val_data = [subjects_list[i] for i in idx_val]\n",
    "        #inputs_val = np.empty((0,224,224,3),dtype='uint8')  \n",
    "        #targets_val = np.empty((0,NUM_CLASSES),dtype='uint8') \n",
    "        #for ii in range(len(val_data)):\n",
    "        #    inputs_val = np.concatenate((inputs_val, val_data[ii][0]),axis=0)\n",
    "        #    targets_val = np.concatenate((targets_val, val_data[ii][1]),axis=0)    \n",
    "        \n",
    "        # CROSS VALIDATION\n",
    "        # loo = KFold(n_splits=10)\n",
    "        loo = LeaveOneOut()\n",
    "        fold = 1   \n",
    "        for idx_train, idx_test in loo.split(list(range(VAL_TRAIN_ID))):\n",
    "            \n",
    "            # initlize variables    \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            print(\"Fold %d of %d\" %(fold, loo.get_n_splits(list(range(VAL_TRAIN_ID)))))\n",
    "            capture_dict[fold] = {}\n",
    "            #\n",
    "            valid_loss, valid_accuracy = [], []\n",
    "            train_loss, train_accuracy = [], []\n",
    "            test_loss, test_accuracy = [], []\n",
    "            \n",
    "            #INTO TRAIN \n",
    "            train_data = [subjects_list[i] for i in idx_train]\n",
    "            inputs_train_ep = np.empty((0,224,224,3),dtype='uint8')  \n",
    "            #targets_train_ep = np.empty((0,NUM_CLASSES),dtype='uint8') \n",
    "            #for ii in range(len(train_data)):\n",
    "            #    inputs_train_ep = np.concatenate((inputs_train_ep, train_data[ii][0]),axis=0)\n",
    "            #    targets_train_ep = np.concatenate((targets_train_ep, train_data[ii][1]),axis=0)\n",
    "            \n",
    "            #INTO TEST \n",
    "            test_data = [subjects_list[i] for i in idx_test]\n",
    "            inputs_test = np.empty((0,224,224,3),dtype='uint8')  \n",
    "            targets_test = np.empty((0,NUM_CLASSES),dtype='uint8') \n",
    "            for ii in range(len(test_data)):\n",
    "                inputs_test = np.concatenate((inputs_test, test_data[ii][0]),axis=0)\n",
    "                targets_test = np.concatenate((targets_test, test_data[ii][1]),axis=0)\n",
    "                        \n",
    "           \n",
    "            # COMPUTE TEST LOSS AND ACCURACY\n",
    "            print('\\tEvaluate test performance')\n",
    "            pred, pred_y_batch = [], []\n",
    "            _iter = 1\n",
    "            #\n",
    "            for x_batch, y_batch in utils.iterate_minibatches(batchsize=BATCH_SIZE, \n",
    "                                                              inputs=inputs_test, \n",
    "                                                              targets=targets_test, \n",
    "                                                              shuffle=False):\n",
    "                _loss,_acc,_pred = sess.run(fetches=[loss, accuracy, prediction],\n",
    "                                            feed_dict={x_pl: x_batch, y_pl: y_batch})\n",
    "                # append prediction\n",
    "                pred += [np.argmax(_pred,1)[ii] for ii in range(len(_pred))]\n",
    "                pred_y_batch += [np.argmax(y_batch,1)[ii] for ii in range(len(y_batch))]\n",
    "                # append mean\n",
    "                test_loss.append(_loss)\n",
    "                test_accuracy.append(_acc)\n",
    "                print(\"\\t\\tminibatch: %d\\tL: %f\\tACCs: %f\" %(_iter,\n",
    "                                                        np.nanmean(test_loss),\n",
    "                                                        np.nanmean(test_accuracy)),end='\\r')\n",
    "                _iter += 1\n",
    "                if _iter == 3: break\n",
    "                # end loop\n",
    "            # calculate performance\n",
    "            cm_test = confusion_matrix(y_pred=pred, \n",
    "                                       y_true=pred_y_batch, \n",
    "                                       labels=list(range(NUM_CLASSES)))\n",
    "\n",
    "            # CAPTURE STATS FOR CURRENT FOLD\n",
    "            capture_dict[fold] = {#'cm_test': cm_test,\n",
    "                                  #'cm_val': cm_val,\n",
    "'idx_train': idx_train,\n",
    "'idx_test': idx_test,\n",
    "'idx_val': idx_val,\n",
    "                                  'train_loss': np.nanmean(train_loss),\n",
    "                                  'train_accuracy': np.nanmean(train_accuracy),\n",
    "                                  'test_loss': np.nanmean(test_loss),\n",
    "                                  'test_accuracy': np.nanmean(test_accuracy),\n",
    "                                  'valid_loss': np.nanmean(valid_loss),\n",
    "                                  'valid_accuracy': np.nanmean(valid_accuracy)} \n",
    "            \n",
    "                \n",
    "           \n",
    "\n",
    "            # increase fold\n",
    "            fold += 1\n",
    "            break\n",
    "        # end loop and traning    \n",
    "        print('\\n... end training loop')\n",
    "        # close session\n",
    "        sess.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.argmax(y_batch,1)[ii] for ii in range(len(y_batch))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'imshow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3b03278a16de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'imshow'"
     ]
    }
   ],
   "source": [
    "s.imshow(inputs_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training loop... \n",
      "\n",
      "Fold 1 of 2\n",
      "\tTrain model\n",
      "\tEvaluate validation performance\n",
      "\t\tminibatch: 2\tL: 2.893645\tACCs: 0.046667\r"
     ]
    }
   ],
   "source": [
    "capture_dict = {}\n",
    "sess = tf.Session(config=config)\n",
    "with sess.as_default():\n",
    "    try:\n",
    "        START_TIME = time.ctime()\n",
    "        MODEL_PATH = \"./models/\"+ FILENAME + \"/Version_\" + VERSION + \"_\" + START_TIME\n",
    "        if not os.path.exists(MODEL_PATH): os.makedirs(MODEL_PATH)\n",
    "        print('Begin training loop... \\n')\n",
    "        \n",
    "        # INTO VALIDATION\n",
    "        idx_val = list(range(VAL_TRAIN_ID, NUM_SUBJECTS))\n",
    "        val_data = [subjects_list[i] for i in idx_val]\n",
    "        inputs_val = np.empty((0,224,224,3),dtype='uint8')  \n",
    "        targets_val = np.empty((0,NUM_CLASSES),dtype='uint8') \n",
    "        for ii in range(len(val_data)):\n",
    "            inputs_val = np.concatenate((inputs_val, val_data[ii][0]),axis=0)\n",
    "            targets_val = np.concatenate((targets_val, val_data[ii][1]),axis=0)    \n",
    "        \n",
    "        # CROSS VALIDATION\n",
    "        # loo = KFold(n_splits=10)\n",
    "        loo = LeaveOneOut()\n",
    "        fold = 1   \n",
    "        for idx_train, idx_test in loo.split(list(range(VAL_TRAIN_ID))):\n",
    "            \n",
    "            # initlize variables    \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            print(\"Fold %d of %d\" %(fold, loo.get_n_splits(list(range(VAL_TRAIN_ID)))))\n",
    "            capture_dict[fold] = {}\n",
    "            #\n",
    "            valid_loss, valid_accuracy = [], []\n",
    "            train_loss, train_accuracy = [], []\n",
    "            test_loss, test_accuracy = [], []\n",
    "            \n",
    "            #INTO TRAIN \n",
    "            train_data = [subjects_list[i] for i in idx_train]\n",
    "            inputs_train_ep = np.empty((0,224,224,3),dtype='uint8')  \n",
    "            targets_train_ep = np.empty((0,NUM_CLASSES),dtype='uint8') \n",
    "            for ii in range(len(train_data)):\n",
    "                inputs_train_ep = np.concatenate((inputs_train_ep, train_data[ii][0]),axis=0)\n",
    "                targets_train_ep = np.concatenate((targets_train_ep, train_data[ii][1]),axis=0)\n",
    "            \n",
    "            #INTO TEST \n",
    "            test_data = [subjects_list[i] for i in idx_test]\n",
    "            inputs_test = np.empty((0,224,224,3),dtype='uint8')  \n",
    "            targets_test = np.empty((0,NUM_CLASSES),dtype='uint8') \n",
    "            for ii in range(len(test_data)):\n",
    "                inputs_test = np.concatenate((inputs_test, test_data[ii][0]),axis=0)\n",
    "                targets_test = np.concatenate((targets_test, test_data[ii][1]),axis=0)\n",
    "                        \n",
    "            # LOOP EPOCHS\n",
    "            print('\\tTrain model')\n",
    "            for epoch in range(MAX_EPOCHS):\n",
    "                break\n",
    "                print('\\tEpoch: ' + str(epoch + 1) + ' of ' + str(MAX_EPOCHS))\n",
    "                # TRAIN\n",
    "                # down sample\n",
    "                inputs_train, targets_train = u_s.down_sample(inputs_=inputs_train_ep, \n",
    "                                                              targets_=targets_train_ep, \n",
    "                                                              no_class=NUM_CLASSES) \n",
    "\n",
    "                _train_loss, _train_accuracy = [], []\n",
    "                _iter = 1\n",
    "                for x_batch, y_batch in utils.iterate_minibatches(batchsize=BATCH_SIZE, \n",
    "                                                                  inputs=inputs_train, \n",
    "                                                                  targets=targets_train, \n",
    "                                                                  shuffle=False):\n",
    "                    #\n",
    "                    _,_loss,_acc = sess.run(fetches=[train_model, loss, accuracy],\n",
    "                                             feed_dict={x_pl: x_batch, y_pl: y_batch})\n",
    "                    \n",
    "                    # append to mini batch\n",
    "                    _train_loss.append(_loss)\n",
    "                    _train_accuracy.append(_acc)                    \n",
    "                    #\n",
    "                    print(\"\\t\\tminibatch: %d\\tL: %f\\tACCs: %f\" %(_iter,\n",
    "                                                            np.nanmean(_train_loss),\n",
    "                                                            np.nanmean(_train_accuracy)),end='\\r')\n",
    "                    _iter += 1\n",
    "                    # end loop\n",
    "                # append mean loss and accuracy\n",
    "                train_loss.append(np.nanmean(_train_loss))\n",
    "                train_accuracy.append(np.nanmean(_train_accuracy))\n",
    "                # end loop\n",
    "\n",
    "            # COMPUTE VALIDATION LOSS AND ACCURACY\n",
    "            print('\\tEvaluate validation performance')\n",
    "            pred, pred_y_batch = [], []\n",
    "            _iter = 1\n",
    "            #\n",
    "            for x_batch, y_batch in utils.iterate_minibatches(batchsize=BATCH_SIZE, \n",
    "                                                              inputs=inputs_val, \n",
    "                                                              targets=targets_val, \n",
    "                                                              shuffle=False):\n",
    "                _loss,_acc,_pred = sess.run(fetches=[loss, accuracy, prediction],\n",
    "                                            feed_dict={x_pl: x_batch, y_pl: y_batch})\n",
    "                # append prediction\n",
    "                pred += [np.argmax(_pred,1)[ii] for ii in range(len(_pred))]\n",
    "                pred_y_batch += [np.argmax(y_batch,1)[ii] for ii in range(len(y_batch))]\n",
    "                # append mean\n",
    "                valid_loss.append(_loss)\n",
    "                valid_accuracy.append(_acc)\n",
    "                print(\"\\t\\tminibatch: %d\\tL: %f\\tACCs: %f\" %(_iter,\n",
    "                                                        np.nanmean(valid_loss),\n",
    "                                                        np.nanmean(valid_accuracy)),end='\\r')\n",
    "                _iter += 1\n",
    "                # end loop\n",
    "            break\n",
    "            # calculate performance\n",
    "            cm_val = confusion_matrix(y_pred=pred, \n",
    "                                      y_true=pred_y_batch, \n",
    "                                      labels=list(range(NUM_CLASSES)))\n",
    "            # COMPUTE TEST LOSS AND ACCURACY\n",
    "            print('\\tEvaluate test performance')\n",
    "            pred, pred_y_batch = [], []\n",
    "            _iter = 1\n",
    "            #\n",
    "            for x_batch, y_batch in utils.iterate_minibatches(batchsize=BATCH_SIZE, \n",
    "                                                              inputs=inputs_test, \n",
    "                                                              targets=targets_test, \n",
    "                                                              shuffle=False):\n",
    "                _loss,_acc,_pred = sess.run(fetches=[loss, accuracy, prediction],\n",
    "                                            feed_dict={x_pl: x_batch, y_pl: y_batch})\n",
    "                # append prediction\n",
    "                pred += [np.argmax(_pred,1)[ii] for ii in range(len(_pred))]\n",
    "                pred_y_batch += [np.argmax(y_batch,1)[ii] for ii in range(len(y_batch))]\n",
    "                # append mean\n",
    "                test_loss.append(_loss)\n",
    "                test_accuracy.append(_acc)\n",
    "                print(\"\\t\\tminibatch: %d\\tL: %f\\tACCs: %f\" %(_iter,\n",
    "                                                        np.nanmean(test_loss),\n",
    "                                                        np.nanmean(test_accuracy)),end='\\r')\n",
    "                _iter += 1\n",
    "                # end loop\n",
    "            # calculate performance\n",
    "            cm_test = confusion_matrix(y_pred=pred, \n",
    "                                       y_true=pred_y_batch, \n",
    "                                       labels=list(range(NUM_CLASSES)))\n",
    "\n",
    "            # CAPTURE STATS FOR CURRENT FOLD\n",
    "            capture_dict[fold] = {'cm_test': cm_test,\n",
    "                                  'cm_val': cm_val,\n",
    "                                  'train_loss': np.nanmean(train_loss),\n",
    "                                  'train_accuracy': np.nanmean(train_accuracy),\n",
    "                                  'test_loss': np.nanmean(test_loss),\n",
    "                                  'test_accuracy': np.nanmean(test_accuracy),\n",
    "                                  'valid_loss': np.nanmean(valid_loss),\n",
    "                                  'valid_accuracy': np.nanmean(valid_accuracy)} \n",
    "            \n",
    "                \n",
    "            # SAVE STATS FOR CURRENT FOLD\n",
    "            np.savez_compressed(MODEL_PATH + \"/capture_dict\", capture_dict)\n",
    "            # tf model\n",
    "            tf_save_path = MODEL_PATH + '/fold_' + str(fold) + '_weigths'\n",
    "            \n",
    "            u_s.save_weights(graph= tf.get_default_graph(), fpath=tf_save_path )\n",
    "            print(\"Model and parameters saved...\")\n",
    "\n",
    "            # increase fold\n",
    "            fold += 1\n",
    "            break\n",
    "        # end loop and traning    \n",
    "        print('\\n... end training loop')\n",
    "        print('started at: ' + START_TIME)\n",
    "        print('ended at:   ' + time.ctime())\n",
    "        # close session\n",
    "        sess.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
