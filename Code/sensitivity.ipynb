{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sen_map notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "#import os\n",
    "#import re\n",
    "#import sys\n",
    "#import time\n",
    "\n",
    "#sys.path.append(os.path.join('.', '..')) \n",
    "#import utils\n",
    "#import utils_DL\n",
    "import utils_s160159 as u_s\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './../Data'\n",
    "logs_path = './logs'\n",
    "save_dir = './../Written work/Article/pics/'\n",
    "SUBJECT_NO = [18,19]\n",
    "NUM_CLASSES = 6\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(\"Loading subject %d...\" %(SUBJECT_NO[0]))\n",
    "#subjects_list_1 = np.load(data_dir + '_dicts' + '/subject_' + str(SUBJECT_NO[0]) + '_dict.npy').item()\n",
    "#print(\"Loading subject %d...\" %(SUBJECT_NO[1]))\n",
    "#subjects_list_2 = np.load(data_dir + '_dicts' + '/subject_' + str(SUBJECT_NO[1]) + '_dict.npy').item()\n",
    "## to dict\n",
    "#subjects_list={}\n",
    "#subjects_list[0] = np.concatenate((subjects_list_1[0], subjects_list_2[0]),axis=0)\n",
    "#subjects_list[1] = np.concatenate((subjects_list_1[1], subjects_list_2[1]),axis=0)\n",
    "#np.save(data_dir + '_dicts/subject_val_dict', subjects_list)\n",
    "subjects_list = np.load(data_dir + '_dicts/subject_val_dict.npy').item()\n",
    "\n",
    "IMAGE_SHAPE = subjects_list[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select first element which forefills this..\n",
    "state_W = np.where(subjects_list[1] == 0)[0][0]\n",
    "state_N1 = np.where(subjects_list[1] == 1)[0][0]\n",
    "state_N2 = np.where(subjects_list[1] == 2)[0][0]\n",
    "state_N3 = np.where(subjects_list[1] == 3)[0][0]\n",
    "state_N4 = np.where(subjects_list[1] == 4)[0][0]\n",
    "state_R = np.where(subjects_list[1] == 5)[0][0]\n",
    "\n",
    "idx = [[state_W],[state_N1],[state_N2],[state_N3],[state_N4],[state_R]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input pic\n",
    "for j in range(len(idx)):\n",
    "    sub_list_avg = np.empty((0,224,224,3))\n",
    "    print(\"Printing sleep stage %d...\" %(j))\n",
    "    for i in idx[j]:\n",
    "        sub_list_avg=np.concatenate((sub_list_avg, [subjects_list[0][i]]), axis=0) \n",
    "    \n",
    "    #\n",
    "    # Calcualte Sensitivity map for each class\n",
    "    sm = np.mean(np.abs(sub_list_avg), axis=0)\n",
    "    #Scale between 0 and 1\n",
    "    sm_min=np.min(sm)\n",
    "    sm_max=np.max(sm)\n",
    "    sensitivity_map = (sm-sm_min)/(sm_max-sm_min)\n",
    "    u_s.cal_sen_map(grad_accum=sensitivity_map,\n",
    "                save_dir=save_dir,\n",
    "                IMAGE_SHAPE=IMAGE_SHAPE, \n",
    "                sen_map_class='clean_' + str(subjects_list[1][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring the master model and  create per class sensitivity map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select first element which forefills this..\n",
    "state_W = np.where(subjects_list[1] == 0)[0]\n",
    "state_N1 = np.where(subjects_list[1] == 1)[0]\n",
    "state_N2 = np.where(subjects_list[1] == 2)[0]\n",
    "state_N3 = np.where(subjects_list[1] == 3)[0]\n",
    "state_N4 = np.where(subjects_list[1] == 4)[0]\n",
    "state_R = np.where(subjects_list[1] == 5)[0]\n",
    "\n",
    "idx = [state_W,state_N1,state_N2,state_N3,state_N4,state_R]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model_path = './models/master/Version_4.0/'\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        # restore model   \n",
    "        #First let's load meta graph and restore weights\n",
    "        saver = tf.train.import_meta_graph(model_path + 'fold_1.ckpt.meta')\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "        graph = tf.get_default_graph()\n",
    "        \n",
    "        x_pl = graph.get_tensor_by_name(name='input_placeholder:0')\n",
    "        y_pl = graph.get_tensor_by_name(name='target_placeholder:0')\n",
    "        cross_entropy = graph.get_tensor_by_name('cross_entropy:0')\n",
    "        sen_map = tf.gradients(cross_entropy, [x_pl], name='sen_map')[0]\n",
    "        \n",
    "        #\n",
    "        for j in range(len(idx)):\n",
    "            #\n",
    "            sub_list_avg = np.empty((0,224,224,3))\n",
    "            # one hot\n",
    "            tmp = np.zeros((1,NUM_CLASSES))\n",
    "            tmp[:,j] = 1\n",
    "            print(\"Printing sleep stage %d...\" %(j))\n",
    "            for i in idx[j]:\n",
    "                \n",
    "                # compute grads\n",
    "                ce = sess.run(fetches=sen_map,\n",
    "                         feed_dict={x_pl: [subjects_list[0][i]],\n",
    "                                    y_pl: tmp})\n",
    "                #\n",
    "                sub_list_avg = np.concatenate((sub_list_avg, ce), axis=0)\n",
    "            \n",
    "            #\n",
    "            # Calcualte Sensitivity map for each class\n",
    "            sm = np.mean(np.abs(sub_list_avg), axis=0)\n",
    "            #Scale between 0 and 1\n",
    "            sm_min=np.min(sm)\n",
    "            sm_max=np.max(sm)\n",
    "            sensitivity_map = (sm-sm_min)/(sm_max-sm_min)\n",
    "            u_s.cal_sen_map(grad_accum=sensitivity_map,\n",
    "                        save_dir=save_dir,\n",
    "                        IMAGE_SHAPE=IMAGE_SHAPE, \n",
    "                        sen_map_class='master_' + str(subjects_list[1][i]))\n",
    "                \n",
    "        # close session\n",
    "        sess.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model_path = './models/rnn/Version_4.0/'\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        # restore model   \n",
    "        #First let's load meta graph and restore weights\n",
    "        saver = tf.train.import_meta_graph(model_path + 'fold_1.ckpt.meta')\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "        graph = tf.get_default_graph()\n",
    "        \n",
    "        x_pl = graph.get_tensor_by_name(name='input_placeholder:0')\n",
    "        y_pl = graph.get_tensor_by_name(name='target_placeholder:0')\n",
    "        cross_entropy = graph.get_tensor_by_name('cross_entropy:0')\n",
    "        sen_map = tf.gradients(cross_entropy, [x_pl], name='sen_map')[0]\n",
    "        \n",
    "        #\n",
    "        for j in range(len(idx)):\n",
    "            #\n",
    "            sub_list_avg = np.empty((0,224,224,3))\n",
    "            # one hot\n",
    "            tmp = np.zeros((BATCH_SIZE, NUM_CLASSES))\n",
    "            tmp[:,j] = 1\n",
    "            \n",
    "            print(\"Printing sleep stage %d...\" %(j))\n",
    "            for i in idx[j]:\n",
    "                \n",
    "                #\n",
    "                tmp_pic = np.zeros((BATCH_SIZE, IMAGE_SHAPE[0], IMAGE_SHAPE[1], IMAGE_SHAPE[2]))\n",
    "                tmp_pic[:] = subjects_list[0][i]\n",
    "                \n",
    "                # compute grads\n",
    "                ce = sess.run(fetches=sen_map,\n",
    "                         feed_dict={x_pl: tmp_pic,\n",
    "                                    y_pl: tmp})\n",
    "                \n",
    "                #\n",
    "                sub_list_avg = np.concatenate((sub_list_avg, ce[0:1]), axis=0)\n",
    "                \n",
    "            #\n",
    "            # Calcualte Sensitivity map for each class\n",
    "            sm = np.mean(np.abs(sub_list_avg), axis=0)\n",
    "            #Scale between 0 and 1\n",
    "            sm_min=np.min(sm)\n",
    "            sm_max=np.max(sm)\n",
    "            sensitivity_map = (sm-sm_min)/(sm_max-sm_min)\n",
    "            u_s.cal_sen_map(grad_accum=sensitivity_map,\n",
    "                        save_dir=save_dir,\n",
    "                        IMAGE_SHAPE=IMAGE_SHAPE, \n",
    "                        sen_map_class='rnn_' + str(subjects_list[1][i]))\n",
    "                \n",
    "        # close session\n",
    "        sess.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
