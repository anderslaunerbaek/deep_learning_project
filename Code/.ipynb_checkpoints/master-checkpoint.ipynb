{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VERSION = '0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function \n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.join('.', '..')) \n",
    "import utils\n",
    "import utils_DL\n",
    "import utils_s160159 as u_s\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import sklearn.datasets\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = './../Data_1'\n",
    "logs_path = './logs'\n",
    "# NUM_SUBJECTS = len([ff for ff in os.listdir(data_dir) if '_img_fpz' in ff])\n",
    "NUM_SUBJECTS = 5 # 20\n",
    "NUM_CLASSES = 6\n",
    "\n",
    "# TRAIN_IDX = list(range(NUM_SUBJECTS))\n",
    "# VAL_IDX = list(range(NUM_SUBJECTS))\n",
    "VAL_PER = 0.05\n",
    "VAL_TRAIN_ID = NUM_SUBJECTS-int(NUM_SUBJECTS * VAL_PER)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 1...\n",
      "Loading subject 2...\n",
      "Loading subject 3...\n",
      "Loading subject 4...\n",
      "Loading subject 5...\n"
     ]
    }
   ],
   "source": [
    "#Load all subjects into memory\n",
    "subjects_list = []\n",
    "for i in range(1,NUM_SUBJECTS+1):\n",
    "    print(\"Loading subject %d...\" %(i))\n",
    "    inputs_night1, targets_night1, _  = u_s.load_spectrograms(data_path=data_dir, subject_id=i, night_id=1)\n",
    "    if(i!=20):\n",
    "        inputs_night2, targets_night2, _  = u_s.load_spectrograms(data_path=data_dir, subject_id=i, night_id=2)\n",
    "    else:\n",
    "        inputs_night2 = np.empty((0,224,224,3),dtype='uint8')\n",
    "        targets_night2 = np.empty((0,NUM_CLASSES),dtype='uint8')           \n",
    "\n",
    "    current_inputs = np.concatenate((inputs_night1,inputs_night2),axis=0)\n",
    "    current_targets = np.concatenate((targets_night1, targets_night2),axis=0)\n",
    "    \n",
    "    subjects_list.append([current_inputs, current_targets])\n",
    "    \n",
    "    #mean_images = np.zeros((5,224,224,3))\n",
    "    #mean_images[0,] = np.mean(current_inputs[current_targets==0,], axis=0)\n",
    "    #mean_images[1,] = np.mean(current_inputs[current_targets==1,], axis=0)\n",
    "    #mean_images[2,] = np.mean(current_inputs[current_targets==2,], axis=0)\n",
    "    #mean_images[3,] = np.mean(current_inputs[np.logical_or(current_targets==3, current_targets==4),], axis=0)\n",
    "    #mean_images[4,] = np.mean(current_inputs[current_targets==5,], axis=0)\n",
    "    #subjects_list.append([current_inputs, current_targets, mean_images])\n",
    "    \n",
    "    \n",
    "# extract image shapes\n",
    "IMAGE_SHAPE = subjects_list[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hyperameters\n",
    "HEIGTH, WIDTH, NCHANNELS = IMAGE_SHAPE[1], IMAGE_SHAPE[2], IMAGE_SHAPE[3]\n",
    "PADDING = 'same'\n",
    "\n",
    "#global_step = tf.Variable(0, trainable=False, name='global_step_traning')\n",
    "\n",
    "#MINI_BATCHES = tf.Variable(250, trainable=False, name='size_of_mini_batch')\n",
    "#DECAY_STEPS = tf.Variable(250, trainable=False, name='number_of_decay_steps')\n",
    "# L_RATE_START = tf.Variable(10e-5, trainable=False, name='learning_rate_start', dtype='float32')\n",
    "# BETA_1 = tf.Variable(0.9, trainable=False, name='learning_rate_first_moment') # first moment\n",
    "# BETA_2 = tf.Variable(0.999, trainable=False, name='learning_rate_second_moment') # second moment\n",
    "\n",
    "# L_RATE = tf.Variable((tf.train.exponential_decay(learning_rate=L_RATE_START,\n",
    "#                            global_step=global_step,\n",
    "#                            decay_rate=BETA_1,\n",
    "#                            decay_steps=DECAY_STEPS,\n",
    "#                            staircase=True) + \n",
    "#           tf.train.exponential_decay(learning_rate=L_RATE_START,\n",
    "#                            global_step=global_step,\n",
    "#                            decay_rate=BETA_2,\n",
    "#                            decay_steps=DECAY_STEPS,\n",
    "#                            staircase=True)),\n",
    "#                      trainable=False, name='learning_rate')\n",
    "# L_RATE = tf.cast(L_RATE, dtype='float32_ref', name='learning_rate')\n",
    "\n",
    "\n",
    "L_RATE = 10e-5\n",
    "EPS = 1e-8\n",
    "TRAIN_FEATURES = False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://www.cs.toronto.edu/~frossard/vgg16/vgg16.py\n",
    "\n",
    "# Load the weights into memory\n",
    "weights_dict = np.load(data_dir + '/' + 'vgg16_weights.npz', encoding='bytes')\n",
    "\n",
    "def tf_conv2d(inputs, name, trainable=TRAIN_FEATURES):\n",
    "    with tf.name_scope(name) as scope:\n",
    "        kernel = tf.get_variable(shape=weights_dict[name + '_W'].shape, \n",
    "                                 initializer=tf.constant_initializer(weights_dict[name + '_W']), \n",
    "                                 name=scope + 'weights', \n",
    "                                 trainable=trainable)\n",
    "        \n",
    "        conv = tf.nn.conv2d(inputs, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        \n",
    "        biases = tf.get_variable(shape=weights_dict[name + '_b'].shape,\n",
    "                                 initializer=tf.constant_initializer(weights_dict[name + '_b']), \n",
    "                                 trainable=trainable, name=scope + 'biases')\n",
    "\n",
    "        return(tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope))\n",
    "        \n",
    "\n",
    "def tf_max_pooling2d(inputs, name):\n",
    "    with tf.name_scope(name) as scope:\n",
    "        return(tf.nn.max_pool(inputs,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name=scope))        \n",
    "\n",
    "def tf_fully_con(inputs, name, shape=[None, 4096]):\n",
    "    with tf.name_scope(name) as scope:    \n",
    "        fc_W = tf.get_variable(shape=shape, \n",
    "                               initializer=tf.contrib.layers.xavier_initializer(dtype=tf.float32,\n",
    "                                                                                seed=22),\n",
    "                               name=scope + 'weights', trainable=True)\n",
    "\n",
    "        fc_b = tf.get_variable(shape=shape[1], \n",
    "                               initializer=tf.random_normal_initializer(mean=0, stddev=0.01, dtype=tf.float32),\n",
    "                               trainable=True, name=scope + 'biases')\n",
    "        \n",
    "        in_flat = tf.reshape(inputs, [-1, shape[0]])\n",
    "        return(tf.nn.relu(tf.nn.bias_add(tf.matmul(in_flat, fc_W), fc_b)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace of the tensors shape as it is propagated through the network.\n",
      "Layer name \t Output size\n",
      "--------------------------------------------\n",
      "x_pl \t\t (?, 224, 224, 3)\n",
      "conv1_1 \t (?, 224, 224, 64)\n",
      "conv1_2 \t (?, 224, 224, 64)\n",
      "pool1 \t\t (?, 112, 112, 64)\n",
      "--------------------------------------------\n",
      "conv2_1 \t (?, 112, 112, 128)\n",
      "conv2_2 \t (?, 112, 112, 128)\n",
      "pool2 \t\t (?, 56, 56, 128)\n",
      "--------------------------------------------\n",
      "conv3_1 \t (?, 56, 56, 256)\n",
      "conv3_2 \t (?, 56, 56, 256)\n",
      "conv3_3 \t (?, 56, 56, 256)\n",
      "pool3 \t\t (?, 28, 28, 256)\n",
      "--------------------------------------------\n",
      "conv4_1 \t (?, 28, 28, 512)\n",
      "conv4_2 \t (?, 28, 28, 512)\n",
      "conv4_3 \t (?, 28, 28, 512)\n",
      "pool4 \t\t (?, 14, 14, 512)\n",
      "--------------------------------------------\n",
      "conv5_1 \t (?, 14, 14, 512)\n",
      "conv5_2 \t (?, 14, 14, 512)\n",
      "conv5_3 \t (?, 14, 14, 512)\n",
      "pool5 \t\t (?, 7, 7, 512)\n",
      "--------------------------------------------\n",
      "fc6 \t (?, 4096)\n",
      "fc7 \t (?, 4096)\n",
      "fc8 \t (?, 6)\n",
      "--------------------------------------------\n",
      "out \t (?, 6)\n",
      "--------------------------------------------\n",
      "Model consits of  119570438 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "# init model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# init placeholders\n",
    "x_pl = tf.placeholder(tf.float32, [None, HEIGTH, WIDTH, NCHANNELS], name='input_placeholder')\n",
    "y_pl = tf.placeholder(tf.float32, [None, NUM_CLASSES], name='target_placeholder')\n",
    "\n",
    "\n",
    "print('Trace of the tensors shape as it is propagated through the network.')\n",
    "print('Layer name \\t Output size')\n",
    "print('--------------------------------------------')\n",
    "\n",
    "with tf.variable_scope('VVG16_layer'):\n",
    "    print('x_pl \\t\\t', x_pl.get_shape())\n",
    "    # level one\n",
    "    conv1_1 = tf_conv2d(inputs=x_pl, name='conv1_1')\n",
    "    conv1_2 = tf_conv2d(inputs=conv1_1, name='conv1_2')\n",
    "    pool1 = tf_max_pooling2d(inputs=conv1_2, name='pool1')\n",
    "    print('conv1_1 \\t', conv1_1.get_shape())\n",
    "    print('conv1_2 \\t', conv1_2.get_shape())\n",
    "    print('pool1 \\t\\t', pool1.get_shape())\n",
    "    print('--------------------------------------------')\n",
    "    \n",
    "    # level two\n",
    "    conv2_1 = tf_conv2d(inputs=pool1, name='conv2_1')\n",
    "    conv2_2 = tf_conv2d(inputs=conv2_1, name='conv2_2')\n",
    "    pool2 = tf_max_pooling2d(inputs=conv2_2, name='pool2')\n",
    "    print('conv2_1 \\t', conv2_1.get_shape())\n",
    "    print('conv2_2 \\t', conv2_2.get_shape())\n",
    "    print('pool2 \\t\\t', pool2.get_shape())\n",
    "    print('--------------------------------------------')\n",
    "    \n",
    "    # level three\n",
    "    conv3_1 = tf_conv2d(inputs=pool2, name='conv3_1')\n",
    "    conv3_2 = tf_conv2d(inputs=conv3_1, name='conv3_2')\n",
    "    conv3_3 = tf_conv2d(inputs=conv3_2, name='conv3_3')\n",
    "    pool3 = tf_max_pooling2d(inputs=conv3_3, name='pool_3')\n",
    "    print('conv3_1 \\t', conv3_1.get_shape())\n",
    "    print('conv3_2 \\t', conv3_2.get_shape())\n",
    "    print('conv3_3 \\t', conv3_3.get_shape())\n",
    "    print('pool3 \\t\\t', pool3.get_shape())\n",
    "    print('--------------------------------------------')\n",
    "    \n",
    "    # level four\n",
    "    conv4_1 = tf_conv2d(inputs=pool3, name='conv4_1')\n",
    "    conv4_2 = tf_conv2d(inputs=conv4_1, name='conv4_2')\n",
    "    conv4_3 = tf_conv2d(inputs=conv4_2, name='conv4_3')\n",
    "    pool4 = tf_max_pooling2d(inputs=conv4_3, name='pool_4')\n",
    "    print('conv4_1 \\t', conv4_1.get_shape())\n",
    "    print('conv4_2 \\t', conv4_2.get_shape())\n",
    "    print('conv4_3 \\t', conv4_3.get_shape())\n",
    "    print('pool4 \\t\\t', pool4.get_shape())\n",
    "    print('--------------------------------------------')\n",
    "\n",
    "    # level five\n",
    "    conv5_1 = tf_conv2d(inputs=pool4, name='conv5_1')\n",
    "    conv5_2 = tf_conv2d(inputs=conv5_1, name='conv5_2')\n",
    "    conv5_3 = tf_conv2d(inputs=conv5_2, name='conv5_3')\n",
    "    pool5 = tf_max_pooling2d(inputs=conv5_3, name='pool_5')\n",
    "    print('conv5_1 \\t', conv5_1.get_shape())\n",
    "    print('conv5_2 \\t', conv5_2.get_shape())\n",
    "    print('conv5_3 \\t', conv5_3.get_shape())\n",
    "    print('pool5 \\t\\t', pool5.get_shape())\n",
    "    print('--------------------------------------------')\n",
    "\n",
    "    # level six\n",
    "    fc6 = tf_fully_con(inputs=pool5, name='fc6', shape=[int(np.prod(pool5.get_shape()[1:])), 4096])\n",
    "    fc6_dropout = tf.layers.dropout(inputs=fc6, name='fc6_dropout', rate=0.5)\n",
    "\n",
    "    # level seven\n",
    "    fc7 = tf_fully_con(inputs=fc6_dropout, name='fc7', shape=[4096, 4096])\n",
    "    fc7_dropout = tf.layers.dropout(inputs=fc7, name='fc7_dropout', rate=0.5)\n",
    "\n",
    "    # level eigth\n",
    "    fc8 = tf_fully_con(inputs=fc7_dropout, name='fc7', shape=[4096, NUM_CLASSES])\n",
    "    #fc8 = tf_fully_con(inputs=fc7_dropout, name='fc7', shape=[4096, 1000])\n",
    "    \n",
    "    print('fc6 \\t', fc6.get_shape())\n",
    "    print('fc7 \\t', fc7.get_shape())\n",
    "    print('fc8 \\t', fc8.get_shape()) \n",
    "    print('--------------------------------------------')\n",
    "\n",
    "\n",
    "    \n",
    "with tf.variable_scope('output_layer'):    \n",
    "    l_out = tf.nn.softmax(fc8, name='l_out')\n",
    "    print('out \\t', l_out.get_shape())\n",
    "    print('--------------------------------------------')\n",
    "    \n",
    "print('Model consits of ', utils_DL.num_params(), 'trainable parameters.')\n",
    "## print all the variable names and shapes\n",
    "#for var in tf.global_variables ():\n",
    "#    s = var.name + \" \"*(40-len(var.name))\n",
    "#    print(s, var.value().get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIMISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss_function'):\n",
    "    # computing cross entropy per sample\n",
    "    cross_entropy = -tf.reduce_sum(y_pl * tf.log(l_out + EPS), axis=[1])\n",
    "    # averaging over samples\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "with tf.variable_scope('training'):\n",
    "    # defining our optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=L_RATE)\n",
    "    # applying the gradients\n",
    "    #train_op = optimizer.minimize(cross_entropy, global_step=global_step)\n",
    "    train_op = optimizer.minimize(cross_entropy)\n",
    "    \n",
    "with tf.variable_scope('performance'):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    prediction = tf.one_hot(tf.argmax(l_out, axis=1), depth=NUM_CLASSES)\n",
    "    correct_prediction = tf.equal(tf.argmax(l_out, axis=1), tf.argmax(y_pl, axis=1))\n",
    "    # averaging the one-hot encoded vector\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test flow for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Launch TensorBoard, and visualize the TF graph\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.20)\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    writer = tf.summary.FileWriter(logs_path, sess.graph)\n",
    "    # close session\n",
    "    sess.close()\n",
    "# run in terminal\n",
    "# \"\"\"\n",
    "# python -m webbrowser \"http://localhost:6006/\";\n",
    "# tensorboard --logdir='./logs'\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass    \n",
    "x_batch = subjects_list[0][0][0:2]\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    y_pred = sess.run(fetches=l_out, feed_dict={x_pl: x_batch})\n",
    "\n",
    "assert y_pred.shape == np.zeros((len(x_batch),NUM_CLASSES)).shape, \"ERROR the output shape is not as expected!\" \\\n",
    "        + \" Output shape should be \" + str(l_out.shape) + ' but was ' + str(y_pred.shape)\n",
    "\n",
    "print('Forward pass successful!')\n",
    "\n",
    "# close session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training loop... \n",
      "\n",
      "Fold num 1\tSubject id 1\n",
      "\tEpoch: 1 of 2\n",
      "\t\tEp: 1\t L: 2.049725\t ACCs: 0.360000\n",
      "here\n",
      "\t\tEpoch 0 : Train Loss  2.050, Train acc  0.360,  Valid loss    nan,  Valid acc    nan\n",
      "\tEpoch: 2 of 2\n",
      "\t\tEp: 2\t L: 6.263031\t ACCs: 0.660000\n",
      "\tEvaluate test performance\n",
      "\t\tTest Loss 16.947, Test acc  0.080\n",
      "\t\tTest Loss 17.500, Test acc  0.050\n",
      "\t\tTest Loss 17.807, Test acc  0.033\n",
      "\t\tTest Loss 17.868, Test acc  0.030\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "MAX_EPOCHS = 2\n",
    "VAL_EPOCHS = 10\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "\n",
    "\n",
    "# reset counter \n",
    "fetches_train = [train_op, cross_entropy, accuracy]\n",
    "fetches_valid = [cross_entropy, accuracy]\n",
    "fetches_test = [cross_entropy, accuracy, prediction]\n",
    "\n",
    "capture_dict = {}\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Begin training loop... \\n')\n",
    "    try:\n",
    "        loo = LeaveOneOut()\n",
    "        fold = 1\n",
    "        for idx_tmp, idx_test in loo.split(range(NUM_SUBJECTS)):\n",
    "            capture_dict[fold] = {}\n",
    "            \n",
    "            #\n",
    "            valid_loss, valid_accuracy = [], []\n",
    "            train_loss, train_accuracy = [], []\n",
    "            test_loss, test_accuracy = [], []\n",
    "            \n",
    "            #\n",
    "            print(\"Fold num %d\\tSubject id %d\" %(fold, idx_test + 1))\n",
    "            #f = open('outputs/sleep5_fold'+str(fold), 'w').close()\n",
    "            \n",
    "            idx_train = idx_tmp[0:VAL_TRAIN_ID]\n",
    "            idx_val = idx_tmp[VAL_TRAIN_ID:(NUM_SUBJECTS - 1)]\n",
    "\n",
    "            # INTO TRAIN \n",
    "            train_data = [subjects_list[i] for i in idx_train]\n",
    "            inputs_train = np.empty((0,224,224,3),dtype='uint8')  \n",
    "            targets_train = np.empty((0,NUM_CLASSES),dtype='uint8') \n",
    "            for item in train_data:\n",
    "                inputs_train = np.concatenate((inputs_train, item[0]),axis=0)\n",
    "                targets_train = np.concatenate((targets_train, item[1]),axis=0)\n",
    "            \n",
    "            # INTO VALIDATION\n",
    "            val_data = [subjects_list[i] for i in idx_val]\n",
    "            inputs_val = np.empty((0,224,224,3),dtype='uint8')  \n",
    "            targets_val = np.empty((0,NUM_CLASSES),dtype='uint8')\n",
    "            for item in val_data:\n",
    "                inputs_val = np.concatenate((inputs_val, item[0]),axis=0)\n",
    "                targets_val = np.concatenate((targets_val, item[1]),axis=0)\n",
    "\n",
    "            # INTO TEST\n",
    "            test_data = [subjects_list[i] for i in idx_test]\n",
    "            inputs_test = np.empty((0,224,224,3), dtype='uint8')  \n",
    "            targets_test = np.empty((0,NUM_CLASSES), dtype='uint8')\n",
    "            for item in test_data:\n",
    "                inputs_test = np.concatenate((inputs_test,item[0]), axis=0)\n",
    "                targets_test = np.concatenate((targets_test,item[1]), axis=0) \n",
    "                \n",
    "            # Loop epochs\n",
    "            for epoch in range(MAX_EPOCHS):\n",
    "                print('\\tEpoch: ' + str(epoch + 1) + ' of ' + str(MAX_EPOCHS))\n",
    "                # loop mini batches\n",
    "                _train_loss, _train_accuracy = [], []\n",
    "                for x_batch, y_batch in utils.iterate_minibatches(batchsize=BATCH_SIZE, \n",
    "                                                                  inputs=inputs_train, \n",
    "                                                                  targets=targets_train, shuffle=False):\n",
    "   \n",
    "                    feed_dict_train = {x_pl: x_batch, \n",
    "                                       y_pl: y_batch}\n",
    "                    _, _loss, _acc = sess.run(fetches_train, feed_dict_train)\n",
    "                    #print(\"\\t\\tEp: %d\\t L: %f\\t ACCs: %f\" %(epoch + 1, _loss, _acc))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # append to epoch\n",
    "                    _train_loss.append(_loss)\n",
    "                    _train_accuracy.append(_acc)\n",
    "                    \n",
    "                    break\n",
    "                #\n",
    "                print(\"\\t\\tEp: %d\\t L: %f\\t ACCs: %f\" %(epoch + 1, \n",
    "                                                        np.mean(_train_loss), \n",
    "                                                        np.mean(_train_accuracy)))\n",
    "                \n",
    "                \n",
    "                \n",
    "                # Compute validation loss and accuracy\n",
    "                if epoch % VAL_EPOCHS == 0:\n",
    "                    print('here')\n",
    "                    # append mean\n",
    "                    train_loss.append(np.mean(_train_loss))\n",
    "                    train_accuracy.append(np.mean(_train_accuracy))\n",
    "                    \n",
    "                    #\n",
    "                    for x_batch, y_batch in utils.iterate_minibatches(batchsize=BATCH_SIZE, \n",
    "                                                                      inputs=inputs_val, \n",
    "                                                                      targets=targets_val, shuffle=False):\n",
    "                        #\n",
    "                        feed_dict_valid = {x_pl: x_batch, y_pl: y_batch}\n",
    "                        _loss, _acc = sess.run(fetches_valid, feed_dict_valid)\n",
    "                        # append mean\n",
    "                        valid_loss.append(_loss)\n",
    "                        valid_accuracy.append(_acc)\n",
    "                    \n",
    "                    #\n",
    "                    print(\"\\t\\tEpoch {} : Train Loss {:6.3f}, Train acc {:6.3f},  Valid loss {:6.3f},  Valid acc {:6.3f}\".format(epoch, train_loss[-1], train_accuracy[-1], valid_loss[-1], valid_accuracy[-1]))\n",
    "                \n",
    "            # Loop test\n",
    "            print('\\tEvaluate test performance')\n",
    "            pred = np.empty((0,NUM_CLASSES),dtype='uint8') \n",
    "            for x_batch, y_batch in utils.iterate_minibatches(batchsize=BATCH_SIZE, \n",
    "                                                              inputs=inputs_test, \n",
    "                                                              targets=targets_test, shuffle=False):\n",
    "\n",
    "                feed_dict_test = {x_pl: x_batch, y_pl: y_batch}\n",
    "                _loss, _acc, _pred = sess.run(fetches_test, feed_dict_test)\n",
    "            \n",
    "                # append prediction\n",
    "                pred = np.concatenate((pred, _pred),axis=0)\n",
    "                test_loss.append(_loss)\n",
    "                test_accuracy.append(_acc)\n",
    "                print('\\t\\tTest Loss {:6.3f}, Test acc {:6.3f}'.format(np.mean(test_loss), \n",
    "                                                                   np.mean(test_accuracy)))\n",
    "            \n",
    "            # calculate performance\n",
    "            cm = confusion_matrix(y_pred=np.where(pred ==1)[1], \n",
    "                                  y_true=np.where(targets_test ==1)[1], labels=list(range(NUM_CLASSES)))\n",
    "            capture_dict[fold] = {'cm': cm, \n",
    "                                  'train_loss': np.mean(train_loss),\n",
    "                                  'train_accuracy': np.mean(train_accuracy),\n",
    "                                  'test_loss': np.mean(test_loss),\n",
    "                                  'test_accuracy': np.mean(test_accuracy),\n",
    "                                  'valid_loss': np.mean(valid_loss),\n",
    "                                  'valid_accuracy': np.mean(valid_accuracy)} \n",
    "            \n",
    "                \n",
    "            # increase fold\n",
    "            fold += 1\n",
    "            print('\\n... end training loop')\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value VVG16_layer/VVG16_layer/conv1_1/biases\n\t [[Node: save_2/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_2/Const_0_0, save_2/SaveV2/tensor_names, save_2/SaveV2/shape_and_slices, VVG16_layer/VVG16_layer/conv1_1/biases, VVG16_layer/VVG16_layer/conv1_1/weights, VVG16_layer/VVG16_layer/conv1_2/biases, VVG16_layer/VVG16_layer/conv1_2/weights, VVG16_layer/VVG16_layer/conv2_1/biases, VVG16_layer/VVG16_layer/conv2_1/weights, VVG16_layer/VVG16_layer/conv2_2/biases, VVG16_layer/VVG16_layer/conv2_2/weights, VVG16_layer/VVG16_layer/conv3_1/biases, VVG16_layer/VVG16_layer/conv3_1/weights, VVG16_layer/VVG16_layer/conv3_2/biases, VVG16_layer/VVG16_layer/conv3_2/weights, VVG16_layer/VVG16_layer/conv3_3/biases, VVG16_layer/VVG16_layer/conv3_3/weights, VVG16_layer/VVG16_layer/conv4_1/biases, VVG16_layer/VVG16_layer/conv4_1/weights, VVG16_layer/VVG16_layer/conv4_2/biases, VVG16_layer/VVG16_layer/conv4_2/weights, VVG16_layer/VVG16_layer/conv4_3/biases, VVG16_layer/VVG16_layer/conv4_3/weights, VVG16_layer/VVG16_layer/conv5_1/biases, VVG16_layer/VVG16_layer/conv5_1/weights, VVG16_layer/VVG16_layer/conv5_2/biases, VVG16_layer/VVG16_layer/conv5_2/weights, VVG16_layer/VVG16_layer/conv5_3/biases, VVG16_layer/VVG16_layer/conv5_3/weights, VVG16_layer/VVG16_layer/fc6/biases, VVG16_layer/VVG16_layer/fc6/weights, VVG16_layer/VVG16_layer/fc7/biases, VVG16_layer/VVG16_layer/fc7/weights, VVG16_layer/VVG16_layer/fc7_1/biases, VVG16_layer/VVG16_layer/fc7_1/weights, training/VVG16_layer/VVG16_layer/fc6/biases/Adam, training/VVG16_layer/VVG16_layer/fc6/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc6/weights/Adam, training/VVG16_layer/VVG16_layer/fc6/weights/Adam_1, training/VVG16_layer/VVG16_layer/fc7/biases/Adam, training/VVG16_layer/VVG16_layer/fc7/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc7/weights/Adam, training/VVG16_layer/VVG16_layer/fc7/weights/Adam_1, training/VVG16_layer/VVG16_layer/fc7_1/biases/Adam, training/VVG16_layer/VVG16_layer/fc7_1/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc7_1/weights/Adam, training/VVG16_layer/VVG16_layer/fc7_1/weights/Adam_1, training/beta1_power, training/beta2_power)]]\n\nCaused by op 'save_2/SaveV2', defined at:\n  File \"/Users/anders/anaconda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/anders/anaconda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-77-cd19728b6558>\", line 5, in <module>\n    save_path = tf.train.Saver().save(sess, \"./models/model.ckpt\")\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1140, in __init__\n    self.build()\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 686, in build\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 276, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 219, in save_op\n    tensors)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 768, in save_v2\n    tensors=tensors, name=name)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value VVG16_layer/VVG16_layer/conv1_1/biases\n\t [[Node: save_2/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_2/Const_0_0, save_2/SaveV2/tensor_names, save_2/SaveV2/shape_and_slices, VVG16_layer/VVG16_layer/conv1_1/biases, VVG16_layer/VVG16_layer/conv1_1/weights, VVG16_layer/VVG16_layer/conv1_2/biases, VVG16_layer/VVG16_layer/conv1_2/weights, VVG16_layer/VVG16_layer/conv2_1/biases, VVG16_layer/VVG16_layer/conv2_1/weights, VVG16_layer/VVG16_layer/conv2_2/biases, VVG16_layer/VVG16_layer/conv2_2/weights, VVG16_layer/VVG16_layer/conv3_1/biases, VVG16_layer/VVG16_layer/conv3_1/weights, VVG16_layer/VVG16_layer/conv3_2/biases, VVG16_layer/VVG16_layer/conv3_2/weights, VVG16_layer/VVG16_layer/conv3_3/biases, VVG16_layer/VVG16_layer/conv3_3/weights, VVG16_layer/VVG16_layer/conv4_1/biases, VVG16_layer/VVG16_layer/conv4_1/weights, VVG16_layer/VVG16_layer/conv4_2/biases, VVG16_layer/VVG16_layer/conv4_2/weights, VVG16_layer/VVG16_layer/conv4_3/biases, VVG16_layer/VVG16_layer/conv4_3/weights, VVG16_layer/VVG16_layer/conv5_1/biases, VVG16_layer/VVG16_layer/conv5_1/weights, VVG16_layer/VVG16_layer/conv5_2/biases, VVG16_layer/VVG16_layer/conv5_2/weights, VVG16_layer/VVG16_layer/conv5_3/biases, VVG16_layer/VVG16_layer/conv5_3/weights, VVG16_layer/VVG16_layer/fc6/biases, VVG16_layer/VVG16_layer/fc6/weights, VVG16_layer/VVG16_layer/fc7/biases, VVG16_layer/VVG16_layer/fc7/weights, VVG16_layer/VVG16_layer/fc7_1/biases, VVG16_layer/VVG16_layer/fc7_1/weights, training/VVG16_layer/VVG16_layer/fc6/biases/Adam, training/VVG16_layer/VVG16_layer/fc6/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc6/weights/Adam, training/VVG16_layer/VVG16_layer/fc6/weights/Adam_1, training/VVG16_layer/VVG16_layer/fc7/biases/Adam, training/VVG16_layer/VVG16_layer/fc7/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc7/weights/Adam, training/VVG16_layer/VVG16_layer/fc7/weights/Adam_1, training/VVG16_layer/VVG16_layer/fc7_1/biases/Adam, training/VVG16_layer/VVG16_layer/fc7_1/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc7_1/weights/Adam, training/VVG16_layer/VVG16_layer/fc7_1/weights/Adam_1, training/beta1_power, training/beta2_power)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anders/anaconda/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value VVG16_layer/VVG16_layer/conv1_1/biases\n\t [[Node: save_2/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_2/Const_0_0, save_2/SaveV2/tensor_names, save_2/SaveV2/shape_and_slices, VVG16_layer/VVG16_layer/conv1_1/biases, VVG16_layer/VVG16_layer/conv1_1/weights, VVG16_layer/VVG16_layer/conv1_2/biases, VVG16_layer/VVG16_layer/conv1_2/weights, VVG16_layer/VVG16_layer/conv2_1/biases, VVG16_layer/VVG16_layer/conv2_1/weights, VVG16_layer/VVG16_layer/conv2_2/biases, VVG16_layer/VVG16_layer/conv2_2/weights, VVG16_layer/VVG16_layer/conv3_1/biases, VVG16_layer/VVG16_layer/conv3_1/weights, VVG16_layer/VVG16_layer/conv3_2/biases, VVG16_layer/VVG16_layer/conv3_2/weights, VVG16_layer/VVG16_layer/conv3_3/biases, VVG16_layer/VVG16_layer/conv3_3/weights, VVG16_layer/VVG16_layer/conv4_1/biases, VVG16_layer/VVG16_layer/conv4_1/weights, VVG16_layer/VVG16_layer/conv4_2/biases, VVG16_layer/VVG16_layer/conv4_2/weights, VVG16_layer/VVG16_layer/conv4_3/biases, VVG16_layer/VVG16_layer/conv4_3/weights, VVG16_layer/VVG16_layer/conv5_1/biases, VVG16_layer/VVG16_layer/conv5_1/weights, VVG16_layer/VVG16_layer/conv5_2/biases, VVG16_layer/VVG16_layer/conv5_2/weights, VVG16_layer/VVG16_layer/conv5_3/biases, VVG16_layer/VVG16_layer/conv5_3/weights, VVG16_layer/VVG16_layer/fc6/biases, VVG16_layer/VVG16_layer/fc6/weights, VVG16_layer/VVG16_layer/fc7/biases, VVG16_layer/VVG16_layer/fc7/weights, VVG16_layer/VVG16_layer/fc7_1/biases, VVG16_layer/VVG16_layer/fc7_1/weights, training/VVG16_layer/VVG16_layer/fc6/biases/Adam, training/VVG16_layer/VVG16_layer/fc6/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc6/weights/Adam, training/VVG16_layer/VVG16_layer/fc6/weights/Adam_1, training/VVG16_layer/VVG16_layer/fc7/biases/Adam, training/VVG16_layer/VVG16_layer/fc7/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc7/weights/Adam, training/VVG16_layer/VVG16_layer/fc7/weights/Adam_1, training/VVG16_layer/VVG16_layer/fc7_1/biases/Adam, training/VVG16_layer/VVG16_layer/fc7_1/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc7_1/weights/Adam, training/VVG16_layer/VVG16_layer/fc7_1/weights/Adam_1, training/beta1_power, training/beta2_power)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-cd19728b6558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# variables to disk.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpu_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./models/model.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved in file: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1488\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[1;32m   1489\u001b[0m                   save_path))\n\u001b[0;32m-> 1490\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1472\u001b[0m         model_checkpoint_path = sess.run(\n\u001b[1;32m   1473\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m             {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1475\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwrite_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value VVG16_layer/VVG16_layer/conv1_1/biases\n\t [[Node: save_2/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_2/Const_0_0, save_2/SaveV2/tensor_names, save_2/SaveV2/shape_and_slices, VVG16_layer/VVG16_layer/conv1_1/biases, VVG16_layer/VVG16_layer/conv1_1/weights, VVG16_layer/VVG16_layer/conv1_2/biases, VVG16_layer/VVG16_layer/conv1_2/weights, VVG16_layer/VVG16_layer/conv2_1/biases, VVG16_layer/VVG16_layer/conv2_1/weights, VVG16_layer/VVG16_layer/conv2_2/biases, VVG16_layer/VVG16_layer/conv2_2/weights, VVG16_layer/VVG16_layer/conv3_1/biases, VVG16_layer/VVG16_layer/conv3_1/weights, VVG16_layer/VVG16_layer/conv3_2/biases, VVG16_layer/VVG16_layer/conv3_2/weights, VVG16_layer/VVG16_layer/conv3_3/biases, VVG16_layer/VVG16_layer/conv3_3/weights, VVG16_layer/VVG16_layer/conv4_1/biases, VVG16_layer/VVG16_layer/conv4_1/weights, VVG16_layer/VVG16_layer/conv4_2/biases, VVG16_layer/VVG16_layer/conv4_2/weights, VVG16_layer/VVG16_layer/conv4_3/biases, VVG16_layer/VVG16_layer/conv4_3/weights, VVG16_layer/VVG16_layer/conv5_1/biases, VVG16_layer/VVG16_layer/conv5_1/weights, VVG16_layer/VVG16_layer/conv5_2/biases, VVG16_layer/VVG16_layer/conv5_2/weights, VVG16_layer/VVG16_layer/conv5_3/biases, VVG16_layer/VVG16_layer/conv5_3/weights, VVG16_layer/VVG16_layer/fc6/biases, VVG16_layer/VVG16_layer/fc6/weights, VVG16_layer/VVG16_layer/fc7/biases, VVG16_layer/VVG16_layer/fc7/weights, VVG16_layer/VVG16_layer/fc7_1/biases, VVG16_layer/VVG16_layer/fc7_1/weights, training/VVG16_layer/VVG16_layer/fc6/biases/Adam, training/VVG16_layer/VVG16_layer/fc6/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc6/weights/Adam, training/VVG16_layer/VVG16_layer/fc6/weights/Adam_1, training/VVG16_layer/VVG16_layer/fc7/biases/Adam, training/VVG16_layer/VVG16_layer/fc7/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc7/weights/Adam, training/VVG16_layer/VVG16_layer/fc7/weights/Adam_1, training/VVG16_layer/VVG16_layer/fc7_1/biases/Adam, training/VVG16_layer/VVG16_layer/fc7_1/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc7_1/weights/Adam, training/VVG16_layer/VVG16_layer/fc7_1/weights/Adam_1, training/beta1_power, training/beta2_power)]]\n\nCaused by op 'save_2/SaveV2', defined at:\n  File \"/Users/anders/anaconda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/anders/anaconda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-77-cd19728b6558>\", line 5, in <module>\n    save_path = tf.train.Saver().save(sess, \"./models/model.ckpt\")\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1140, in __init__\n    self.build()\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 686, in build\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 276, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 219, in save_op\n    tensors)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 768, in save_v2\n    tensors=tensors, name=name)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/anders/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value VVG16_layer/VVG16_layer/conv1_1/biases\n\t [[Node: save_2/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_2/Const_0_0, save_2/SaveV2/tensor_names, save_2/SaveV2/shape_and_slices, VVG16_layer/VVG16_layer/conv1_1/biases, VVG16_layer/VVG16_layer/conv1_1/weights, VVG16_layer/VVG16_layer/conv1_2/biases, VVG16_layer/VVG16_layer/conv1_2/weights, VVG16_layer/VVG16_layer/conv2_1/biases, VVG16_layer/VVG16_layer/conv2_1/weights, VVG16_layer/VVG16_layer/conv2_2/biases, VVG16_layer/VVG16_layer/conv2_2/weights, VVG16_layer/VVG16_layer/conv3_1/biases, VVG16_layer/VVG16_layer/conv3_1/weights, VVG16_layer/VVG16_layer/conv3_2/biases, VVG16_layer/VVG16_layer/conv3_2/weights, VVG16_layer/VVG16_layer/conv3_3/biases, VVG16_layer/VVG16_layer/conv3_3/weights, VVG16_layer/VVG16_layer/conv4_1/biases, VVG16_layer/VVG16_layer/conv4_1/weights, VVG16_layer/VVG16_layer/conv4_2/biases, VVG16_layer/VVG16_layer/conv4_2/weights, VVG16_layer/VVG16_layer/conv4_3/biases, VVG16_layer/VVG16_layer/conv4_3/weights, VVG16_layer/VVG16_layer/conv5_1/biases, VVG16_layer/VVG16_layer/conv5_1/weights, VVG16_layer/VVG16_layer/conv5_2/biases, VVG16_layer/VVG16_layer/conv5_2/weights, VVG16_layer/VVG16_layer/conv5_3/biases, VVG16_layer/VVG16_layer/conv5_3/weights, VVG16_layer/VVG16_layer/fc6/biases, VVG16_layer/VVG16_layer/fc6/weights, VVG16_layer/VVG16_layer/fc7/biases, VVG16_layer/VVG16_layer/fc7/weights, VVG16_layer/VVG16_layer/fc7_1/biases, VVG16_layer/VVG16_layer/fc7_1/weights, training/VVG16_layer/VVG16_layer/fc6/biases/Adam, training/VVG16_layer/VVG16_layer/fc6/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc6/weights/Adam, training/VVG16_layer/VVG16_layer/fc6/weights/Adam_1, training/VVG16_layer/VVG16_layer/fc7/biases/Adam, training/VVG16_layer/VVG16_layer/fc7/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc7/weights/Adam, training/VVG16_layer/VVG16_layer/fc7/weights/Adam_1, training/VVG16_layer/VVG16_layer/fc7_1/biases/Adam, training/VVG16_layer/VVG16_layer/fc7_1/biases/Adam_1, training/VVG16_layer/VVG16_layer/fc7_1/weights/Adam, training/VVG16_layer/VVG16_layer/fc7_1/weights/Adam_1, training/beta1_power, training/beta2_power)]]\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/versions/r1.0/programmers_guide/variables\n",
    "# Later, launch the model, initialize the variables, do some work, save the\n",
    "# variables to disk.\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    save_path = tf.train.Saver(var_list=tf.trainable_variables()).save(sess, \"./models/model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
